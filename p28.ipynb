{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from gtts import gTTS\n",
        "\n",
        "# === One-syllable words list ===\n",
        "one_syllable_words = [\n",
        "    \"cat\", \"dog\", \"sun\", \"run\", \"walk\", \"jump\", \"red\", \"blue\", \"black\", \"green\",\n",
        "    \"fish\", \"ship\", \"hat\", \"bat\", \"rat\", \"mat\", \"cap\", \"map\", \"trap\", \"snap\",\n",
        "    \"clap\", \"trip\", \"grip\", \"drip\", \"flip\", \"clip\", \"kick\", \"pick\", \"lick\", \"stick\",\n",
        "    \"brick\", \"click\", \"quick\", \"thick\", \"thin\", \"win\", \"sin\", \"bin\", \"pin\", \"tin\",\n",
        "    \"fin\", \"rim\", \"dim\", \"jam\", \"ram\", \"dam\", \"bam\", \"mad\", \"sad\", \"bad\", \"glad\",\n",
        "    \"pad\", \"lad\", \"dad\", \"bag\", \"tag\", \"lag\", \"nag\", \"zag\", \"tug\", \"hug\", \"bug\",\n",
        "    \"jug\", \"mug\", \"rug\", \"dug\", \"cup\", \"pup\", \"up\", \"mud\", \"bud\", \"stud\", \"cut\",\n",
        "    \"gut\", \"nut\", \"but\", \"shut\", \"put\", \"pot\", \"dot\", \"got\", \"hot\", \"not\", \"rot\",\n",
        "    \"lot\", \"shot\", \"plot\", \"spot\", \"drop\", \"stop\", \"crop\", \"hop\", \"pop\", \"top\"\n",
        "]\n",
        "\n",
        "# === Create folder ===\n",
        "os.makedirs(\"words\", exist_ok=True)\n",
        "\n",
        "# === Generate audio files ===\n",
        "for word in one_syllable_words:\n",
        "    tts = gTTS(text=word, lang='en')\n",
        "    file_path = os.path.join(\"words\", f\"{word}.mp3\")\n",
        "    tts.save(file_path)\n",
        "    print(f\"✅ Saved: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjPKYn1OwjUD",
        "outputId": "bd24404d-564e-47e8-c7d4-ffbe1a494416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: words/cat.mp3\n",
            "✅ Saved: words/dog.mp3\n",
            "✅ Saved: words/sun.mp3\n",
            "✅ Saved: words/run.mp3\n",
            "✅ Saved: words/walk.mp3\n",
            "✅ Saved: words/jump.mp3\n",
            "✅ Saved: words/red.mp3\n",
            "✅ Saved: words/blue.mp3\n",
            "✅ Saved: words/black.mp3\n",
            "✅ Saved: words/green.mp3\n",
            "✅ Saved: words/fish.mp3\n",
            "✅ Saved: words/ship.mp3\n",
            "✅ Saved: words/hat.mp3\n",
            "✅ Saved: words/bat.mp3\n",
            "✅ Saved: words/rat.mp3\n",
            "✅ Saved: words/mat.mp3\n",
            "✅ Saved: words/cap.mp3\n",
            "✅ Saved: words/map.mp3\n",
            "✅ Saved: words/trap.mp3\n",
            "✅ Saved: words/snap.mp3\n",
            "✅ Saved: words/clap.mp3\n",
            "✅ Saved: words/trip.mp3\n",
            "✅ Saved: words/grip.mp3\n",
            "✅ Saved: words/drip.mp3\n",
            "✅ Saved: words/flip.mp3\n",
            "✅ Saved: words/clip.mp3\n",
            "✅ Saved: words/kick.mp3\n",
            "✅ Saved: words/pick.mp3\n",
            "✅ Saved: words/lick.mp3\n",
            "✅ Saved: words/stick.mp3\n",
            "✅ Saved: words/brick.mp3\n",
            "✅ Saved: words/click.mp3\n",
            "✅ Saved: words/quick.mp3\n",
            "✅ Saved: words/thick.mp3\n",
            "✅ Saved: words/thin.mp3\n",
            "✅ Saved: words/win.mp3\n",
            "✅ Saved: words/sin.mp3\n",
            "✅ Saved: words/bin.mp3\n",
            "✅ Saved: words/pin.mp3\n",
            "✅ Saved: words/tin.mp3\n",
            "✅ Saved: words/fin.mp3\n",
            "✅ Saved: words/rim.mp3\n",
            "✅ Saved: words/dim.mp3\n",
            "✅ Saved: words/jam.mp3\n",
            "✅ Saved: words/ram.mp3\n",
            "✅ Saved: words/dam.mp3\n",
            "✅ Saved: words/bam.mp3\n",
            "✅ Saved: words/mad.mp3\n",
            "✅ Saved: words/sad.mp3\n",
            "✅ Saved: words/bad.mp3\n",
            "✅ Saved: words/glad.mp3\n",
            "✅ Saved: words/pad.mp3\n",
            "✅ Saved: words/lad.mp3\n",
            "✅ Saved: words/dad.mp3\n",
            "✅ Saved: words/bag.mp3\n",
            "✅ Saved: words/tag.mp3\n",
            "✅ Saved: words/lag.mp3\n",
            "✅ Saved: words/nag.mp3\n",
            "✅ Saved: words/zag.mp3\n",
            "✅ Saved: words/tug.mp3\n",
            "✅ Saved: words/hug.mp3\n",
            "✅ Saved: words/bug.mp3\n",
            "✅ Saved: words/jug.mp3\n",
            "✅ Saved: words/mug.mp3\n",
            "✅ Saved: words/rug.mp3\n",
            "✅ Saved: words/dug.mp3\n",
            "✅ Saved: words/cup.mp3\n",
            "✅ Saved: words/pup.mp3\n",
            "✅ Saved: words/up.mp3\n",
            "✅ Saved: words/mud.mp3\n",
            "✅ Saved: words/bud.mp3\n",
            "✅ Saved: words/stud.mp3\n",
            "✅ Saved: words/cut.mp3\n",
            "✅ Saved: words/gut.mp3\n",
            "✅ Saved: words/nut.mp3\n",
            "✅ Saved: words/but.mp3\n",
            "✅ Saved: words/shut.mp3\n",
            "✅ Saved: words/put.mp3\n",
            "✅ Saved: words/pot.mp3\n",
            "✅ Saved: words/dot.mp3\n",
            "✅ Saved: words/got.mp3\n",
            "✅ Saved: words/hot.mp3\n",
            "✅ Saved: words/not.mp3\n",
            "✅ Saved: words/rot.mp3\n",
            "✅ Saved: words/lot.mp3\n",
            "✅ Saved: words/shot.mp3\n",
            "✅ Saved: words/plot.mp3\n",
            "✅ Saved: words/spot.mp3\n",
            "✅ Saved: words/drop.mp3\n",
            "✅ Saved: words/stop.mp3\n",
            "✅ Saved: words/crop.mp3\n",
            "✅ Saved: words/hop.mp3\n",
            "✅ Saved: words/pop.mp3\n",
            "✅ Saved: words/top.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g69IHs1lIhXx"
      },
      "source": [
        "The next thing we'll do is to create a melody as midi. A melody with 1000 notes. We'll extend this when we want more data, but at the moment this corresponds to 2 generated songs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwzDkeOPEfkl",
        "outputId": "7ac0bb62-bb0c-484d-f909-58d1ef36efb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎵 Saved as melody.mid\n"
          ]
        }
      ],
      "source": [
        "from midiutil import MIDIFile\n",
        "import random\n",
        "\n",
        "# === Helper Functions ===\n",
        "def get_minor_scale_with_octaves(root_midi):\n",
        "    \"\"\"Return natural minor scale in octaves 3, 4, and 5.\"\"\"\n",
        "    intervals = [0, 2, 3, 5, 7, 8, 10]\n",
        "    base = [root_midi + i for i in intervals]\n",
        "    return base + [n - 12 for n in base] + [n + 12 for n in base]\n",
        "\n",
        "# === CONFIG ===\n",
        "mf = MIDIFile(1)\n",
        "track = 0\n",
        "channel = 0\n",
        "volume = 100\n",
        "default_tempo = 90\n",
        "mf.addTempo(track, 0, default_tempo)\n",
        "\n",
        "durations = [0.25, 0.5, 1, 2]           # 16th, 8th, quarter, half\n",
        "weights =   [1440, 80, 53, 27]          # scaled to preserve 90% 16th bias\n",
        "\n",
        "def choose_duration():\n",
        "    return random.choices(durations, weights=weights, k=1)[0]\n",
        "\n",
        "current_time = 0\n",
        "three_minutes_beats = 3 * default_tempo\n",
        "\n",
        "# === Initialize first key and tempo\n",
        "current_tempo = default_tempo\n",
        "current_root = 59  # B3\n",
        "current_scale = get_minor_scale_with_octaves(current_root)\n",
        "mf.addTempo(track, current_time, current_tempo)\n",
        "\n",
        "# === Melody Generation ===\n",
        "last_pitch = random.choice(current_scale)  # start with any note\n",
        "note_count = 0\n",
        "\n",
        "while note_count < 200:\n",
        "    duration = random.choice(durations)\n",
        "\n",
        "    # Style switch if 3 minutes of MIDI time (in beats) have passed\n",
        "    if current_time >= three_minutes_beats:\n",
        "        current_time = round(current_time, 2)\n",
        "        current_tempo = random.randint(20, 200)\n",
        "        current_root = random.randint(48, 72)\n",
        "        current_scale = get_minor_scale_with_octaves(current_root)\n",
        "        mf.addTempo(track, current_time, current_tempo)\n",
        "        three_minutes_beats = current_time + 3 * current_tempo\n",
        "\n",
        "    # Filter notes within ±7 semitones of last pitch\n",
        "    candidates = [p for p in current_scale if abs(p - last_pitch) <= 7]\n",
        "    if not candidates:\n",
        "        candidates = [last_pitch]  # fallback in case of no valid options\n",
        "\n",
        "    pitch = random.choice(candidates)\n",
        "\n",
        "    # Optional tied note\n",
        "    if random.random() < 0.3:\n",
        "        tie = random.choice(durations)\n",
        "        duration += tie\n",
        "\n",
        "    mf.addNote(track, channel, pitch, current_time, duration, volume)\n",
        "    current_time += duration\n",
        "    last_pitch = pitch\n",
        "    note_count += 1\n",
        "\n",
        "# === Save MIDI\n",
        "with open(\"melody.mid\", \"wb\") as f:\n",
        "    mf.writeFile(f)\n",
        "\n",
        "print(\"🎵 Saved as melody.mid\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Kvl4karLCPV"
      },
      "source": [
        "Now we have both lyrics and a melody. In this step we want to combine them together to create the training data. For now we're just doing the rhythm. We're still working on adding pitch correctly. But except for pitch we can extend the data to be as much as we like and the step after the next we actually train the ai."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HawVqNaDLZy0",
        "outputId": "93fc71bd-a974-4309-9b1f-0d23a7121ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎵 Loading MIDI file: melody.mid\n",
            "🎹 MIDI loaded. Total duration: 0.04 hours\n",
            "📝 Preparing blank WAV file: melody.wav\n",
            "🎶 Processing note 1: Pitch 55, Time 0.00s → 0.17s\n",
            "🎶 Processing note 101: Pitch 57, Time 72.67s → 72.83s\n",
            "✅ Synthesis complete. 200 notes written to: melody.wav\n",
            "📏 Normalizing audio...\n",
            "✅ Normalization done.\n",
            "🔄 Converting to MP3: melody.mp3\n",
            "🧹 Removing intermediate WAV file: melody.wav\n",
            "🎉 Done! MP3 saved to: melody.mp3\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import soundfile as sf\n",
        "import subprocess\n",
        "import librosa\n",
        "\n",
        "# === CONFIG ===\n",
        "midi_path = \"melody.mid\"\n",
        "output_path = \"melody.mp3\"\n",
        "wav_path = output_path.replace(\".mp3\", \".wav\")\n",
        "sr = 22050\n",
        "words_folder = \"words\"\n",
        "\n",
        "pretty_midi.pretty_midi.MAX_TICK = 1e10\n",
        "\n",
        "# === Load MIDI ===\n",
        "print(f\"🎵 Loading MIDI file: {midi_path}\")\n",
        "midi_data = pretty_midi.PrettyMIDI(midi_path)\n",
        "duration_sec = midi_data.get_end_time() + 1\n",
        "print(f\"🎹 MIDI loaded. Total duration: {duration_sec / 3600:.2f} hours\")\n",
        "\n",
        "# === Prepare blank WAV file ===\n",
        "print(f\"📝 Preparing blank WAV file: {wav_path}\")\n",
        "with sf.SoundFile(wav_path, mode='w', samplerate=sr, channels=1, subtype='PCM_16'):\n",
        "    pass\n",
        "\n",
        "# === Get word file paths ===\n",
        "word_files = [os.path.join(words_folder, f) for f in os.listdir(words_folder) if f.endswith(\".mp3\")]\n",
        "if len(word_files) == 0:\n",
        "    raise RuntimeError(\"No word audio files found in 'words' folder!\")\n",
        "\n",
        "# === Process notes ===\n",
        "note_count = 0\n",
        "for instrument in midi_data.instruments:\n",
        "    for note in instrument.notes:\n",
        "        note_count += 1\n",
        "        if note_count % 100 == 1:\n",
        "            print(f\"🎶 Processing note {note_count}: Pitch {note.pitch}, Time {note.start:.2f}s → {note.end:.2f}s\")\n",
        "\n",
        "        duration = note.end - note.start\n",
        "        start_sample = int(note.start * sr)\n",
        "\n",
        "        # Choose a random word audio file\n",
        "        word_path = random.choice(word_files)\n",
        "        word_audio, _ = librosa.load(word_path, sr=sr)\n",
        "\n",
        "        # Time-stretch to fit note duration\n",
        "        current_duration = len(word_audio) / sr\n",
        "        stretch_factor = current_duration / duration\n",
        "        word_stretched = librosa.effects.time_stretch(word_audio, rate=1/stretch_factor)\n",
        "\n",
        "        # Vary volume randomly between 40% and 100%\n",
        "        volume = random.uniform(0.4, 1.0)\n",
        "        word_stretched *= volume\n",
        "\n",
        "        # Apply fade-in/out to avoid clicks\n",
        "        fade_samples = int(0.02 * sr)\n",
        "        envelope = np.ones_like(word_stretched)\n",
        "        envelope[:fade_samples] = np.linspace(0, 1, fade_samples)\n",
        "        envelope[-fade_samples:] = np.linspace(1, 0, fade_samples)\n",
        "        word_stretched *= envelope\n",
        "\n",
        "        # Write to WAV\n",
        "        with sf.SoundFile(wav_path, mode='r+') as f:\n",
        "            current_frames = f.frames\n",
        "            # Pad with silence if needed\n",
        "            if start_sample > current_frames:\n",
        "                f.seek(0, sf.SEEK_END)\n",
        "                f.write(np.zeros(start_sample - current_frames))\n",
        "            # Write at correct time location\n",
        "            f.seek(start_sample)\n",
        "            f.write(word_stretched)\n",
        "\n",
        "print(f\"✅ Synthesis complete. {note_count} notes written to: {wav_path}\")\n",
        "\n",
        "# === Normalize ===\n",
        "print(\"📏 Normalizing audio...\")\n",
        "y, _ = librosa.load(wav_path, sr=sr)\n",
        "y = y / np.max(np.abs(y))\n",
        "sf.write(wav_path, y, sr)\n",
        "print(\"✅ Normalization done.\")\n",
        "\n",
        "# === Convert to MP3 ===\n",
        "print(f\"🔄 Converting to MP3: {output_path}\")\n",
        "subprocess.call([\"ffmpeg\", \"-y\", \"-i\", wav_path, output_path])\n",
        "\n",
        "# === Clean up ===\n",
        "print(f\"🧹 Removing intermediate WAV file: {wav_path}\")\n",
        "os.remove(wav_path)\n",
        "print(f\"🎉 Done! MP3 saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DkE1S71ag0H",
        "outputId": "5c91236c-2ef5-4869-be82-162d43984200"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Done! Extracted 200 onsets from melody.mid.\n",
            "🎧 Audio file: melody.mp3\n",
            "📄 Onset labels saved to: onsets.txt (in seconds)\n",
            "🧠 You can now use this for training your onset detection model.\n"
          ]
        }
      ],
      "source": [
        "midi = pretty_midi.PrettyMIDI(\"melody.mid\")\n",
        "onset_times = []\n",
        "\n",
        "for inst in midi.instruments:\n",
        "    if inst.is_drum:\n",
        "        continue\n",
        "    for note in inst.notes:\n",
        "        onset_times.append(note.start)  # start time in SECONDS, with tempo applied\n",
        "\n",
        "# Save to file\n",
        "with open(\"onsets.txt\", \"w\") as f:\n",
        "    for t in sorted(onset_times):\n",
        "        f.write(f\"{t:.6f}\\n\")\n",
        "\n",
        "print(f\"\\n✅ Done! Extracted {len(onset_times)} onsets from melody.mid.\")\n",
        "print(\"🎧 Audio file: melody.mp3\")\n",
        "print(\"📄 Onset labels saved to: onsets.txt (in seconds)\")\n",
        "print(\"🧠 You can now use this for training your onset detection model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag8e2ryOUT1_"
      },
      "source": [
        "This is the part where we actually train the ai to eventually become very good at onset detection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKS-FTvZMdjM",
        "outputId": "4921c10e-8140-4ebc-bd19-6df337d55c19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting training...\n",
            "📘 Epoch 1/20000000\n",
            "✅ Avg Loss: 0.509180 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 2/20000000\n",
            "✅ Avg Loss: 0.487469 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 3/20000000\n",
            "✅ Avg Loss: 0.481399 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 4/20000000\n",
            "✅ Avg Loss: 0.475846 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 5/20000000\n",
            "✅ Avg Loss: 0.472476 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 6/20000000\n",
            "✅ Avg Loss: 0.470986 | 🎯 New Best F1: 0.0044 🏆\n",
            "💾 Model saved: onset_model_epoch6.pth\n",
            "📘 Epoch 7/20000000\n",
            "✅ Avg Loss: 0.462866 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 8/20000000\n",
            "✅ Avg Loss: 0.464269 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 9/20000000\n",
            "✅ Avg Loss: 0.454953 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 10/20000000\n",
            "✅ Avg Loss: 0.456438 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 11/20000000\n",
            "✅ Avg Loss: 0.455833 | 🎯 New Best F1: 0.0428 🏆\n",
            "💾 Model saved: onset_model_epoch11.pth\n",
            "📘 Epoch 12/20000000\n",
            "✅ Avg Loss: 0.454939 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 13/20000000\n",
            "✅ Avg Loss: 0.451311 | 🎯 New Best F1: 0.0625 🏆\n",
            "💾 Model saved: onset_model_epoch13.pth\n",
            "📘 Epoch 14/20000000\n",
            "✅ Avg Loss: 0.454402 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 15/20000000\n",
            "✅ Avg Loss: 0.446950 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 16/20000000\n",
            "✅ Avg Loss: 0.449113 | 🎯 F1 Score (±40ms): 0.0169\n",
            "📘 Epoch 17/20000000\n",
            "✅ Avg Loss: 0.446299 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 18/20000000\n",
            "✅ Avg Loss: 0.450720 | 🎯 F1 Score (±40ms): 0.0174\n",
            "📘 Epoch 19/20000000\n",
            "✅ Avg Loss: 0.445030 | 🎯 F1 Score (±40ms): 0.0322\n",
            "📘 Epoch 20/20000000\n",
            "✅ Avg Loss: 0.443582 | 🎯 F1 Score (±40ms): 0.0130\n",
            "📘 Epoch 21/20000000\n",
            "✅ Avg Loss: 0.445446 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 22/20000000\n",
            "✅ Avg Loss: 0.444569 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 23/20000000\n",
            "✅ Avg Loss: 0.444414 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 24/20000000\n",
            "✅ Avg Loss: 0.440400 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 25/20000000\n",
            "✅ Avg Loss: 0.444513 | 🎯 F1 Score (±40ms): 0.0400\n",
            "📘 Epoch 26/20000000\n",
            "✅ Avg Loss: 0.443391 | 🎯 New Best F1: 0.1002 🏆\n",
            "💾 Model saved: onset_model_epoch26.pth\n",
            "📘 Epoch 27/20000000\n",
            "✅ Avg Loss: 0.443726 | 🎯 F1 Score (±40ms): 0.0846\n",
            "📘 Epoch 28/20000000\n",
            "✅ Avg Loss: 0.440946 | 🎯 F1 Score (±40ms): 0.0451\n",
            "📘 Epoch 29/20000000\n",
            "✅ Avg Loss: 0.441128 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 30/20000000\n",
            "✅ Avg Loss: 0.444142 | 🎯 F1 Score (±40ms): 0.0835\n",
            "📘 Epoch 31/20000000\n",
            "✅ Avg Loss: 0.437243 | 🎯 F1 Score (±40ms): 0.0681\n",
            "📘 Epoch 32/20000000\n",
            "✅ Avg Loss: 0.438960 | 🎯 F1 Score (±40ms): 0.0088\n",
            "📘 Epoch 33/20000000\n",
            "✅ Avg Loss: 0.440331 | 🎯 New Best F1: 0.1422 🏆\n",
            "💾 Model saved: onset_model_epoch33.pth\n",
            "📘 Epoch 34/20000000\n",
            "✅ Avg Loss: 0.441115 | 🎯 F1 Score (±40ms): 0.0924\n",
            "📘 Epoch 35/20000000\n",
            "✅ Avg Loss: 0.439066 | 🎯 F1 Score (±40ms): 0.0606\n",
            "📘 Epoch 36/20000000\n",
            "✅ Avg Loss: 0.434750 | 🎯 F1 Score (±40ms): 0.1221\n",
            "📘 Epoch 37/20000000\n",
            "✅ Avg Loss: 0.435572 | 🎯 F1 Score (±40ms): 0.0696\n",
            "📘 Epoch 38/20000000\n",
            "✅ Avg Loss: 0.434715 | 🎯 F1 Score (±40ms): 0.0705\n",
            "📘 Epoch 39/20000000\n",
            "✅ Avg Loss: 0.431880 | 🎯 F1 Score (±40ms): 0.0378\n",
            "📘 Epoch 40/20000000\n",
            "✅ Avg Loss: 0.439296 | 🎯 F1 Score (±40ms): 0.0172\n",
            "📘 Epoch 41/20000000\n",
            "✅ Avg Loss: 0.433228 | 🎯 F1 Score (±40ms): 0.0131\n",
            "📘 Epoch 42/20000000\n",
            "✅ Avg Loss: 0.440471 | 🎯 F1 Score (±40ms): 0.1375\n",
            "📘 Epoch 43/20000000\n",
            "✅ Avg Loss: 0.436420 | 🎯 F1 Score (±40ms): 0.0414\n",
            "📘 Epoch 44/20000000\n",
            "✅ Avg Loss: 0.435149 | 🎯 F1 Score (±40ms): 0.0599\n",
            "📘 Epoch 45/20000000\n",
            "✅ Avg Loss: 0.440270 | 🎯 F1 Score (±40ms): 0.0000\n",
            "📘 Epoch 46/20000000\n",
            "✅ Avg Loss: 0.437971 | 🎯 F1 Score (±40ms): 0.0606\n",
            "📘 Epoch 47/20000000\n",
            "✅ Avg Loss: 0.434480 | 🎯 F1 Score (±40ms): 0.1078\n",
            "📘 Epoch 48/20000000\n",
            "✅ Avg Loss: 0.438880 | 🎯 F1 Score (±40ms): 0.0044\n",
            "📘 Epoch 49/20000000\n",
            "✅ Avg Loss: 0.433243 | 🎯 New Best F1: 0.1504 🏆\n",
            "💾 Model saved: onset_model_epoch49.pth\n",
            "📘 Epoch 50/20000000\n",
            "✅ Avg Loss: 0.434122 | 🎯 F1 Score (±40ms): 0.1162\n",
            "📘 Epoch 51/20000000\n",
            "✅ Avg Loss: 0.432178 | 🎯 F1 Score (±40ms): 0.0634\n",
            "📘 Epoch 52/20000000\n",
            "✅ Avg Loss: 0.431897 | 🎯 F1 Score (±40ms): 0.0891\n",
            "📘 Epoch 53/20000000\n",
            "✅ Avg Loss: 0.432380 | 🎯 New Best F1: 0.1527 🏆\n",
            "💾 Model saved: onset_model_epoch53.pth\n",
            "📘 Epoch 54/20000000\n",
            "✅ Avg Loss: 0.430313 | 🎯 F1 Score (±40ms): 0.0696\n",
            "📘 Epoch 55/20000000\n",
            "✅ Avg Loss: 0.432565 | 🎯 F1 Score (±40ms): 0.0420\n",
            "📘 Epoch 56/20000000\n",
            "✅ Avg Loss: 0.439323 | 🎯 F1 Score (±40ms): 0.0886\n",
            "📘 Epoch 57/20000000\n",
            "✅ Avg Loss: 0.430448 | 🎯 F1 Score (±40ms): 0.0939\n",
            "📘 Epoch 58/20000000\n",
            "✅ Avg Loss: 0.436700 | 🎯 F1 Score (±40ms): 0.0341\n",
            "📘 Epoch 59/20000000\n",
            "✅ Avg Loss: 0.431276 | 🎯 F1 Score (±40ms): 0.0842\n",
            "📘 Epoch 60/20000000\n",
            "✅ Avg Loss: 0.432372 | 🎯 F1 Score (±40ms): 0.0802\n",
            "📘 Epoch 61/20000000\n",
            "✅ Avg Loss: 0.436151 | 🎯 F1 Score (±40ms): 0.0044\n",
            "📘 Epoch 62/20000000\n",
            "✅ Avg Loss: 0.433861 | 🎯 F1 Score (±40ms): 0.1129\n",
            "📘 Epoch 63/20000000\n",
            "✅ Avg Loss: 0.431967 | 🎯 F1 Score (±40ms): 0.1070\n",
            "📘 Epoch 64/20000000\n",
            "✅ Avg Loss: 0.427908 | 🎯 New Best F1: 0.1839 🏆\n",
            "💾 Model saved: onset_model_epoch64.pth\n",
            "📘 Epoch 65/20000000\n",
            "✅ Avg Loss: 0.431454 | 🎯 F1 Score (±40ms): 0.0762\n",
            "📘 Epoch 66/20000000\n",
            "✅ Avg Loss: 0.428706 | 🎯 F1 Score (±40ms): 0.0699\n",
            "📘 Epoch 67/20000000\n",
            "✅ Avg Loss: 0.429373 | 🎯 F1 Score (±40ms): 0.1433\n",
            "📘 Epoch 68/20000000\n",
            "✅ Avg Loss: 0.428131 | 🎯 F1 Score (±40ms): 0.0732\n",
            "📘 Epoch 69/20000000\n",
            "✅ Avg Loss: 0.428590 | 🎯 F1 Score (±40ms): 0.0982\n",
            "📘 Epoch 70/20000000\n",
            "✅ Avg Loss: 0.433375 | 🎯 F1 Score (±40ms): 0.0980\n",
            "📘 Epoch 71/20000000\n",
            "✅ Avg Loss: 0.431467 | 🎯 F1 Score (±40ms): 0.0821\n",
            "📘 Epoch 72/20000000\n",
            "✅ Avg Loss: 0.429008 | 🎯 F1 Score (±40ms): 0.1248\n",
            "📘 Epoch 73/20000000\n",
            "✅ Avg Loss: 0.429013 | 🎯 F1 Score (±40ms): 0.1390\n",
            "📘 Epoch 74/20000000\n",
            "✅ Avg Loss: 0.431075 | 🎯 F1 Score (±40ms): 0.1313\n",
            "📘 Epoch 75/20000000\n",
            "✅ Avg Loss: 0.430603 | 🎯 F1 Score (±40ms): 0.0736\n",
            "📘 Epoch 76/20000000\n",
            "✅ Avg Loss: 0.425982 | 🎯 F1 Score (±40ms): 0.0965\n",
            "📘 Epoch 77/20000000\n",
            "✅ Avg Loss: 0.430619 | 🎯 F1 Score (±40ms): 0.1252\n",
            "📘 Epoch 78/20000000\n",
            "✅ Avg Loss: 0.431042 | 🎯 F1 Score (±40ms): 0.1491\n",
            "📘 Epoch 79/20000000\n",
            "✅ Avg Loss: 0.426993 | 🎯 F1 Score (±40ms): 0.0537\n",
            "📘 Epoch 80/20000000\n",
            "✅ Avg Loss: 0.426587 | 🎯 F1 Score (±40ms): 0.0671\n",
            "📘 Epoch 81/20000000\n",
            "✅ Avg Loss: 0.429456 | 🎯 F1 Score (±40ms): 0.1487\n",
            "📘 Epoch 82/20000000\n",
            "✅ Avg Loss: 0.430956 | 🎯 F1 Score (±40ms): 0.1265\n",
            "📘 Epoch 83/20000000\n",
            "✅ Avg Loss: 0.427094 | 🎯 F1 Score (±40ms): 0.1444\n",
            "📘 Epoch 84/20000000\n",
            "✅ Avg Loss: 0.428641 | 🎯 New Best F1: 0.2202 🏆\n",
            "💾 Model saved: onset_model_epoch84.pth\n",
            "📘 Epoch 85/20000000\n",
            "✅ Avg Loss: 0.430973 | 🎯 F1 Score (±40ms): 0.1058\n",
            "📘 Epoch 86/20000000\n",
            "✅ Avg Loss: 0.430735 | 🎯 F1 Score (±40ms): 0.0980\n",
            "📘 Epoch 87/20000000\n",
            "✅ Avg Loss: 0.428125 | 🎯 F1 Score (±40ms): 0.2149\n",
            "📘 Epoch 88/20000000\n",
            "✅ Avg Loss: 0.430521 | 🎯 F1 Score (±40ms): 0.1160\n",
            "📘 Epoch 89/20000000\n",
            "✅ Avg Loss: 0.428215 | 🎯 New Best F1: 0.2627 🏆\n",
            "💾 Model saved: onset_model_epoch89.pth\n",
            "📘 Epoch 90/20000000\n",
            "✅ Avg Loss: 0.427641 | 🎯 F1 Score (±40ms): 0.2087\n",
            "📘 Epoch 91/20000000\n",
            "✅ Avg Loss: 0.425613 | 🎯 F1 Score (±40ms): 0.0928\n",
            "📘 Epoch 92/20000000\n",
            "✅ Avg Loss: 0.428752 | 🎯 F1 Score (±40ms): 0.0378\n",
            "📘 Epoch 93/20000000\n",
            "✅ Avg Loss: 0.426506 | 🎯 New Best F1: 0.2664 🏆\n",
            "💾 Model saved: onset_model_epoch93.pth\n",
            "📘 Epoch 94/20000000\n",
            "✅ Avg Loss: 0.432735 | 🎯 F1 Score (±40ms): 0.1083\n",
            "📘 Epoch 95/20000000\n",
            "✅ Avg Loss: 0.428163 | 🎯 F1 Score (±40ms): 0.0904\n",
            "📘 Epoch 96/20000000\n",
            "✅ Avg Loss: 0.429545 | 🎯 F1 Score (±40ms): 0.0982\n",
            "📘 Epoch 97/20000000\n",
            "✅ Avg Loss: 0.425389 | 🎯 F1 Score (±40ms): 0.0744\n",
            "📘 Epoch 98/20000000\n",
            "✅ Avg Loss: 0.424855 | 🎯 F1 Score (±40ms): 0.2267\n",
            "📘 Epoch 99/20000000\n",
            "✅ Avg Loss: 0.425325 | 🎯 F1 Score (±40ms): 0.1369\n",
            "📘 Epoch 100/20000000\n",
            "✅ Avg Loss: 0.428937 | 🎯 F1 Score (±40ms): 0.2003\n",
            "📘 Epoch 101/20000000\n",
            "✅ Avg Loss: 0.428646 | 🎯 F1 Score (±40ms): 0.0213\n",
            "📘 Epoch 102/20000000\n",
            "✅ Avg Loss: 0.429081 | 🎯 F1 Score (±40ms): 0.1088\n",
            "📘 Epoch 103/20000000\n",
            "✅ Avg Loss: 0.430786 | 🎯 F1 Score (±40ms): 0.1572\n",
            "📘 Epoch 104/20000000\n",
            "✅ Avg Loss: 0.427799 | 🎯 F1 Score (±40ms): 0.1769\n",
            "📘 Epoch 105/20000000\n",
            "✅ Avg Loss: 0.430244 | 🎯 F1 Score (±40ms): 0.1945\n",
            "📘 Epoch 106/20000000\n",
            "✅ Avg Loss: 0.429140 | 🎯 F1 Score (±40ms): 0.1773\n",
            "📘 Epoch 107/20000000\n",
            "✅ Avg Loss: 0.426553 | 🎯 F1 Score (±40ms): 0.1340\n",
            "📘 Epoch 108/20000000\n",
            "✅ Avg Loss: 0.428550 | 🎯 F1 Score (±40ms): 0.1027\n",
            "📘 Epoch 109/20000000\n",
            "✅ Avg Loss: 0.427464 | 🎯 F1 Score (±40ms): 0.2314\n",
            "📘 Epoch 110/20000000\n",
            "✅ Avg Loss: 0.428640 | 🎯 F1 Score (±40ms): 0.1000\n",
            "📘 Epoch 111/20000000\n",
            "✅ Avg Loss: 0.428322 | 🎯 F1 Score (±40ms): 0.1545\n",
            "📘 Epoch 112/20000000\n",
            "✅ Avg Loss: 0.426079 | 🎯 F1 Score (±40ms): 0.1358\n",
            "📘 Epoch 113/20000000\n",
            "✅ Avg Loss: 0.426777 | 🎯 F1 Score (±40ms): 0.1706\n",
            "📘 Epoch 114/20000000\n",
            "✅ Avg Loss: 0.425095 | 🎯 F1 Score (±40ms): 0.1146\n",
            "📘 Epoch 115/20000000\n",
            "✅ Avg Loss: 0.427204 | 🎯 F1 Score (±40ms): 0.0851\n",
            "📘 Epoch 116/20000000\n",
            "✅ Avg Loss: 0.425132 | 🎯 F1 Score (±40ms): 0.0929\n",
            "📘 Epoch 117/20000000\n",
            "✅ Avg Loss: 0.426358 | 🎯 F1 Score (±40ms): 0.2490\n",
            "📘 Epoch 118/20000000\n",
            "✅ Avg Loss: 0.424280 | 🎯 F1 Score (±40ms): 0.2160\n",
            "📘 Epoch 119/20000000\n",
            "✅ Avg Loss: 0.426711 | 🎯 F1 Score (±40ms): 0.1235\n",
            "📘 Epoch 120/20000000\n",
            "✅ Avg Loss: 0.426670 | 🎯 F1 Score (±40ms): 0.0972\n",
            "📘 Epoch 121/20000000\n",
            "✅ Avg Loss: 0.425382 | 🎯 F1 Score (±40ms): 0.1039\n",
            "📘 Epoch 122/20000000\n",
            "✅ Avg Loss: 0.425556 | 🎯 F1 Score (±40ms): 0.2654\n",
            "📘 Epoch 123/20000000\n",
            "✅ Avg Loss: 0.428147 | 🎯 F1 Score (±40ms): 0.1787\n",
            "📘 Epoch 124/20000000\n",
            "✅ Avg Loss: 0.424494 | 🎯 F1 Score (±40ms): 0.1463\n",
            "📘 Epoch 125/20000000\n",
            "✅ Avg Loss: 0.429704 | 🎯 F1 Score (±40ms): 0.1235\n",
            "📘 Epoch 126/20000000\n",
            "✅ Avg Loss: 0.430937 | 🎯 F1 Score (±40ms): 0.0605\n",
            "📘 Epoch 127/20000000\n",
            "✅ Avg Loss: 0.425184 | 🎯 F1 Score (±40ms): 0.0892\n",
            "📘 Epoch 128/20000000\n",
            "✅ Avg Loss: 0.426301 | 🎯 F1 Score (±40ms): 0.1426\n",
            "📘 Epoch 129/20000000\n",
            "✅ Avg Loss: 0.424577 | 🎯 F1 Score (±40ms): 0.2025\n",
            "📘 Epoch 130/20000000\n",
            "✅ Avg Loss: 0.424825 | 🎯 F1 Score (±40ms): 0.1071\n",
            "📘 Epoch 131/20000000\n",
            "✅ Avg Loss: 0.426294 | 🎯 F1 Score (±40ms): 0.2008\n",
            "📘 Epoch 132/20000000\n",
            "✅ Avg Loss: 0.426610 | 🎯 F1 Score (±40ms): 0.1201\n",
            "📘 Epoch 133/20000000\n",
            "✅ Avg Loss: 0.428442 | 🎯 F1 Score (±40ms): 0.1252\n",
            "📘 Epoch 134/20000000\n",
            "✅ Avg Loss: 0.426241 | 🎯 F1 Score (±40ms): 0.1691\n",
            "📘 Epoch 135/20000000\n",
            "✅ Avg Loss: 0.425614 | 🎯 F1 Score (±40ms): 0.0978\n",
            "📘 Epoch 136/20000000\n",
            "✅ Avg Loss: 0.425451 | 🎯 F1 Score (±40ms): 0.1595\n",
            "📘 Epoch 137/20000000\n",
            "✅ Avg Loss: 0.425027 | 🎯 F1 Score (±40ms): 0.1524\n",
            "📘 Epoch 138/20000000\n",
            "✅ Avg Loss: 0.428541 | 🎯 F1 Score (±40ms): 0.1055\n",
            "📘 Epoch 139/20000000\n",
            "✅ Avg Loss: 0.427663 | 🎯 F1 Score (±40ms): 0.1479\n",
            "📘 Epoch 140/20000000\n",
            "✅ Avg Loss: 0.423068 | 🎯 F1 Score (±40ms): 0.1789\n",
            "📘 Epoch 141/20000000\n",
            "✅ Avg Loss: 0.425349 | 🎯 F1 Score (±40ms): 0.1940\n",
            "📘 Epoch 142/20000000\n",
            "✅ Avg Loss: 0.425916 | 🎯 F1 Score (±40ms): 0.2022\n",
            "📘 Epoch 143/20000000\n",
            "✅ Avg Loss: 0.426562 | 🎯 F1 Score (±40ms): 0.2144\n",
            "📘 Epoch 144/20000000\n",
            "✅ Avg Loss: 0.425308 | 🎯 F1 Score (±40ms): 0.2140\n",
            "📘 Epoch 145/20000000\n",
            "✅ Avg Loss: 0.426294 | 🎯 F1 Score (±40ms): 0.1782\n",
            "📘 Epoch 146/20000000\n",
            "✅ Avg Loss: 0.425267 | 🎯 F1 Score (±40ms): 0.2172\n",
            "📘 Epoch 147/20000000\n",
            "✅ Avg Loss: 0.426098 | 🎯 F1 Score (±40ms): 0.1235\n",
            "📘 Epoch 148/20000000\n",
            "✅ Avg Loss: 0.424733 | 🎯 F1 Score (±40ms): 0.1860\n",
            "📘 Epoch 149/20000000\n",
            "✅ Avg Loss: 0.430909 | 🎯 F1 Score (±40ms): 0.2057\n",
            "📘 Epoch 150/20000000\n",
            "✅ Avg Loss: 0.424521 | 🎯 F1 Score (±40ms): 0.2358\n",
            "📘 Epoch 151/20000000\n",
            "✅ Avg Loss: 0.424776 | 🎯 F1 Score (±40ms): 0.2615\n",
            "📘 Epoch 152/20000000\n",
            "✅ Avg Loss: 0.425632 | 🎯 F1 Score (±40ms): 0.2194\n",
            "📘 Epoch 153/20000000\n",
            "✅ Avg Loss: 0.428492 | 🎯 F1 Score (±40ms): 0.0493\n",
            "📘 Epoch 154/20000000\n",
            "✅ Avg Loss: 0.424706 | 🎯 F1 Score (±40ms): 0.1403\n",
            "📘 Epoch 155/20000000\n",
            "✅ Avg Loss: 0.423671 | 🎯 F1 Score (±40ms): 0.1189\n",
            "📘 Epoch 156/20000000\n",
            "✅ Avg Loss: 0.425033 | 🎯 F1 Score (±40ms): 0.1315\n",
            "📘 Epoch 157/20000000\n",
            "✅ Avg Loss: 0.423940 | 🎯 F1 Score (±40ms): 0.1232\n",
            "📘 Epoch 158/20000000\n",
            "✅ Avg Loss: 0.429660 | 🎯 F1 Score (±40ms): 0.1502\n",
            "📘 Epoch 159/20000000\n",
            "✅ Avg Loss: 0.426860 | 🎯 F1 Score (±40ms): 0.1752\n",
            "📘 Epoch 160/20000000\n",
            "✅ Avg Loss: 0.425022 | 🎯 F1 Score (±40ms): 0.1753\n",
            "📘 Epoch 161/20000000\n",
            "✅ Avg Loss: 0.425711 | 🎯 F1 Score (±40ms): 0.1438\n",
            "📘 Epoch 162/20000000\n",
            "✅ Avg Loss: 0.428749 | 🎯 F1 Score (±40ms): 0.2241\n",
            "📘 Epoch 163/20000000\n",
            "✅ Avg Loss: 0.426067 | 🎯 F1 Score (±40ms): 0.2357\n",
            "📘 Epoch 164/20000000\n",
            "✅ Avg Loss: 0.424241 | 🎯 F1 Score (±40ms): 0.1743\n",
            "📘 Epoch 165/20000000\n",
            "✅ Avg Loss: 0.425238 | 🎯 F1 Score (±40ms): 0.1922\n",
            "📘 Epoch 166/20000000\n",
            "✅ Avg Loss: 0.425289 | 🎯 F1 Score (±40ms): 0.2388\n",
            "📘 Epoch 167/20000000\n",
            "✅ Avg Loss: 0.427216 | 🎯 F1 Score (±40ms): 0.2245\n",
            "📘 Epoch 168/20000000\n",
            "✅ Avg Loss: 0.425376 | 🎯 F1 Score (±40ms): 0.2485\n",
            "📘 Epoch 169/20000000\n",
            "✅ Avg Loss: 0.425968 | 🎯 F1 Score (±40ms): 0.1369\n",
            "📘 Epoch 170/20000000\n",
            "✅ Avg Loss: 0.425168 | 🎯 F1 Score (±40ms): 0.1931\n",
            "📘 Epoch 171/20000000\n",
            "✅ Avg Loss: 0.425689 | 🎯 F1 Score (±40ms): 0.1443\n",
            "📘 Epoch 172/20000000\n",
            "✅ Avg Loss: 0.424540 | 🎯 F1 Score (±40ms): 0.1324\n",
            "📘 Epoch 173/20000000\n",
            "✅ Avg Loss: 0.424757 | 🎯 F1 Score (±40ms): 0.2310\n",
            "📘 Epoch 174/20000000\n",
            "✅ Avg Loss: 0.424971 | 🎯 F1 Score (±40ms): 0.1831\n",
            "📘 Epoch 175/20000000\n",
            "✅ Avg Loss: 0.428812 | 🎯 F1 Score (±40ms): 0.1876\n",
            "📘 Epoch 176/20000000\n",
            "✅ Avg Loss: 0.424278 | 🎯 F1 Score (±40ms): 0.2208\n",
            "📘 Epoch 177/20000000\n",
            "✅ Avg Loss: 0.426175 | 🎯 F1 Score (±40ms): 0.1984\n",
            "📘 Epoch 178/20000000\n",
            "✅ Avg Loss: 0.423985 | 🎯 F1 Score (±40ms): 0.0996\n",
            "📘 Epoch 179/20000000\n",
            "✅ Avg Loss: 0.426979 | 🎯 F1 Score (±40ms): 0.1370\n",
            "📘 Epoch 180/20000000\n",
            "✅ Avg Loss: 0.424370 | 🎯 F1 Score (±40ms): 0.1969\n",
            "📘 Epoch 181/20000000\n",
            "✅ Avg Loss: 0.424269 | 🎯 F1 Score (±40ms): 0.2174\n",
            "📘 Epoch 182/20000000\n",
            "✅ Avg Loss: 0.426772 | 🎯 F1 Score (±40ms): 0.2472\n",
            "📘 Epoch 183/20000000\n",
            "✅ Avg Loss: 0.425269 | 🎯 F1 Score (±40ms): 0.2502\n",
            "📘 Epoch 184/20000000\n",
            "✅ Avg Loss: 0.425425 | 🎯 F1 Score (±40ms): 0.1209\n",
            "📘 Epoch 185/20000000\n",
            "✅ Avg Loss: 0.424263 | 🎯 F1 Score (±40ms): 0.1950\n",
            "📘 Epoch 186/20000000\n",
            "✅ Avg Loss: 0.424353 | 🎯 F1 Score (±40ms): 0.2570\n",
            "📘 Epoch 187/20000000\n",
            "✅ Avg Loss: 0.425750 | 🎯 F1 Score (±40ms): 0.2473\n",
            "📘 Epoch 188/20000000\n",
            "✅ Avg Loss: 0.423984 | 🎯 F1 Score (±40ms): 0.2568\n",
            "📘 Epoch 189/20000000\n",
            "✅ Avg Loss: 0.425355 | 🎯 F1 Score (±40ms): 0.2325\n",
            "📘 Epoch 190/20000000\n",
            "✅ Avg Loss: 0.425585 | 🎯 F1 Score (±40ms): 0.0806\n",
            "📘 Epoch 191/20000000\n",
            "✅ Avg Loss: 0.426066 | 🎯 F1 Score (±40ms): 0.2526\n",
            "📘 Epoch 192/20000000\n",
            "✅ Avg Loss: 0.425572 | 🎯 F1 Score (±40ms): 0.0738\n",
            "📘 Epoch 193/20000000\n",
            "✅ Avg Loss: 0.427223 | 🎯 F1 Score (±40ms): 0.2041\n",
            "📘 Epoch 194/20000000\n",
            "✅ Avg Loss: 0.426118 | 🎯 F1 Score (±40ms): 0.1922\n",
            "📘 Epoch 195/20000000\n",
            "✅ Avg Loss: 0.425165 | 🎯 F1 Score (±40ms): 0.1942\n",
            "📘 Epoch 196/20000000\n",
            "✅ Avg Loss: 0.424917 | 🎯 F1 Score (±40ms): 0.1860\n",
            "📘 Epoch 197/20000000\n",
            "✅ Avg Loss: 0.422135 | 🎯 F1 Score (±40ms): 0.1465\n",
            "📘 Epoch 198/20000000\n",
            "✅ Avg Loss: 0.425957 | 🎯 F1 Score (±40ms): 0.2453\n",
            "📘 Epoch 199/20000000\n",
            "✅ Avg Loss: 0.426562 | 🎯 F1 Score (±40ms): 0.1403\n",
            "📘 Epoch 200/20000000\n",
            "✅ Avg Loss: 0.428233 | 🎯 F1 Score (±40ms): 0.1239\n",
            "📘 Epoch 201/20000000\n",
            "✅ Avg Loss: 0.424594 | 🎯 F1 Score (±40ms): 0.1137\n",
            "📘 Epoch 202/20000000\n",
            "✅ Avg Loss: 0.423634 | 🎯 F1 Score (±40ms): 0.2542\n",
            "📘 Epoch 203/20000000\n",
            "✅ Avg Loss: 0.422737 | 🎯 F1 Score (±40ms): 0.1531\n",
            "📘 Epoch 204/20000000\n",
            "✅ Avg Loss: 0.424366 | 🎯 F1 Score (±40ms): 0.1925\n",
            "📘 Epoch 205/20000000\n",
            "✅ Avg Loss: 0.425124 | 🎯 F1 Score (±40ms): 0.0713\n",
            "📘 Epoch 206/20000000\n",
            "✅ Avg Loss: 0.424695 | 🎯 F1 Score (±40ms): 0.2353\n",
            "📘 Epoch 207/20000000\n",
            "✅ Avg Loss: 0.422906 | 🎯 F1 Score (±40ms): 0.1919\n",
            "📘 Epoch 208/20000000\n",
            "✅ Avg Loss: 0.425101 | 🎯 F1 Score (±40ms): 0.1563\n",
            "📘 Epoch 209/20000000\n",
            "✅ Avg Loss: 0.424295 | 🎯 F1 Score (±40ms): 0.1829\n",
            "📘 Epoch 210/20000000\n",
            "✅ Avg Loss: 0.424294 | 🎯 F1 Score (±40ms): 0.1193\n",
            "📘 Epoch 211/20000000\n",
            "✅ Avg Loss: 0.422364 | 🎯 F1 Score (±40ms): 0.1370\n",
            "📘 Epoch 212/20000000\n",
            "✅ Avg Loss: 0.424865 | 🎯 F1 Score (±40ms): 0.1780\n",
            "📘 Epoch 213/20000000\n",
            "✅ Avg Loss: 0.425048 | 🎯 F1 Score (±40ms): 0.2300\n",
            "📘 Epoch 214/20000000\n",
            "✅ Avg Loss: 0.424632 | 🎯 F1 Score (±40ms): 0.1121\n",
            "📘 Epoch 215/20000000\n",
            "✅ Avg Loss: 0.425297 | 🎯 F1 Score (±40ms): 0.1899\n",
            "📘 Epoch 216/20000000\n",
            "✅ Avg Loss: 0.426682 | 🎯 F1 Score (±40ms): 0.1546\n",
            "📘 Epoch 217/20000000\n",
            "✅ Avg Loss: 0.427919 | 🎯 F1 Score (±40ms): 0.2231\n",
            "📘 Epoch 218/20000000\n",
            "✅ Avg Loss: 0.424881 | 🎯 F1 Score (±40ms): 0.0826\n",
            "📘 Epoch 219/20000000\n",
            "✅ Avg Loss: 0.425128 | 🎯 F1 Score (±40ms): 0.2060\n",
            "📘 Epoch 220/20000000\n",
            "✅ Avg Loss: 0.424240 | 🎯 F1 Score (±40ms): 0.1938\n",
            "📘 Epoch 221/20000000\n",
            "✅ Avg Loss: 0.422800 | 🎯 F1 Score (±40ms): 0.1083\n",
            "📘 Epoch 222/20000000\n",
            "✅ Avg Loss: 0.423948 | 🎯 F1 Score (±40ms): 0.2416\n",
            "📘 Epoch 223/20000000\n",
            "✅ Avg Loss: 0.427751 | 🎯 F1 Score (±40ms): 0.2535\n",
            "📘 Epoch 224/20000000\n",
            "✅ Avg Loss: 0.427860 | 🎯 F1 Score (±40ms): 0.2471\n",
            "📘 Epoch 225/20000000\n",
            "✅ Avg Loss: 0.424314 | 🎯 F1 Score (±40ms): 0.1763\n",
            "📘 Epoch 226/20000000\n",
            "✅ Avg Loss: 0.421820 | 🎯 F1 Score (±40ms): 0.2488\n",
            "📘 Epoch 227/20000000\n",
            "✅ Avg Loss: 0.424156 | 🎯 New Best F1: 0.2755 🏆\n",
            "💾 Model saved: onset_model_epoch227.pth\n",
            "📘 Epoch 228/20000000\n",
            "✅ Avg Loss: 0.423336 | 🎯 F1 Score (±40ms): 0.1844\n",
            "📘 Epoch 229/20000000\n",
            "✅ Avg Loss: 0.422830 | 🎯 F1 Score (±40ms): 0.2324\n",
            "📘 Epoch 230/20000000\n",
            "✅ Avg Loss: 0.425041 | 🎯 F1 Score (±40ms): 0.1797\n",
            "📘 Epoch 231/20000000\n",
            "✅ Avg Loss: 0.424531 | 🎯 F1 Score (±40ms): 0.2366\n",
            "📘 Epoch 232/20000000\n",
            "✅ Avg Loss: 0.425860 | 🎯 F1 Score (±40ms): 0.0982\n",
            "📘 Epoch 233/20000000\n",
            "✅ Avg Loss: 0.425147 | 🎯 F1 Score (±40ms): 0.1383\n",
            "📘 Epoch 234/20000000\n",
            "✅ Avg Loss: 0.422489 | 🎯 F1 Score (±40ms): 0.1028\n",
            "📘 Epoch 235/20000000\n",
            "✅ Avg Loss: 0.422409 | 🎯 F1 Score (±40ms): 0.2373\n",
            "📘 Epoch 236/20000000\n",
            "✅ Avg Loss: 0.424723 | 🎯 F1 Score (±40ms): 0.2214\n",
            "📘 Epoch 237/20000000\n",
            "✅ Avg Loss: 0.422776 | 🎯 F1 Score (±40ms): 0.2409\n",
            "📘 Epoch 238/20000000\n",
            "✅ Avg Loss: 0.424596 | 🎯 F1 Score (±40ms): 0.1828\n",
            "📘 Epoch 239/20000000\n",
            "✅ Avg Loss: 0.424158 | 🎯 F1 Score (±40ms): 0.2180\n",
            "📘 Epoch 240/20000000\n",
            "✅ Avg Loss: 0.423163 | 🎯 F1 Score (±40ms): 0.0769\n",
            "📘 Epoch 241/20000000\n",
            "✅ Avg Loss: 0.426569 | 🎯 F1 Score (±40ms): 0.2464\n",
            "📘 Epoch 242/20000000\n",
            "✅ Avg Loss: 0.424306 | 🎯 F1 Score (±40ms): 0.1649\n",
            "📘 Epoch 243/20000000\n",
            "✅ Avg Loss: 0.420561 | 🎯 F1 Score (±40ms): 0.2473\n",
            "📘 Epoch 244/20000000\n",
            "✅ Avg Loss: 0.422043 | 🎯 F1 Score (±40ms): 0.1847\n",
            "📘 Epoch 245/20000000\n",
            "✅ Avg Loss: 0.423609 | 🎯 F1 Score (±40ms): 0.1543\n",
            "📘 Epoch 246/20000000\n",
            "✅ Avg Loss: 0.423739 | 🎯 F1 Score (±40ms): 0.2269\n",
            "📘 Epoch 247/20000000\n",
            "✅ Avg Loss: 0.426139 | 🎯 F1 Score (±40ms): 0.2451\n",
            "📘 Epoch 248/20000000\n",
            "✅ Avg Loss: 0.424647 | 🎯 F1 Score (±40ms): 0.1628\n",
            "📘 Epoch 249/20000000\n",
            "✅ Avg Loss: 0.424338 | 🎯 F1 Score (±40ms): 0.1687\n",
            "📘 Epoch 250/20000000\n",
            "✅ Avg Loss: 0.423853 | 🎯 F1 Score (±40ms): 0.1713\n",
            "📘 Epoch 251/20000000\n",
            "✅ Avg Loss: 0.426162 | 🎯 F1 Score (±40ms): 0.2100\n",
            "📘 Epoch 252/20000000\n",
            "✅ Avg Loss: 0.422729 | 🎯 F1 Score (±40ms): 0.2244\n",
            "📘 Epoch 253/20000000\n",
            "✅ Avg Loss: 0.428408 | 🎯 F1 Score (±40ms): 0.2393\n",
            "📘 Epoch 254/20000000\n",
            "✅ Avg Loss: 0.422728 | 🎯 F1 Score (±40ms): 0.1771\n",
            "📘 Epoch 255/20000000\n",
            "✅ Avg Loss: 0.422225 | 🎯 F1 Score (±40ms): 0.2416\n",
            "📘 Epoch 256/20000000\n",
            "✅ Avg Loss: 0.421155 | 🎯 F1 Score (±40ms): 0.2648\n",
            "📘 Epoch 257/20000000\n",
            "✅ Avg Loss: 0.426380 | 🎯 F1 Score (±40ms): 0.2033\n",
            "📘 Epoch 258/20000000\n",
            "✅ Avg Loss: 0.423830 | 🎯 F1 Score (±40ms): 0.1802\n",
            "📘 Epoch 259/20000000\n",
            "✅ Avg Loss: 0.423655 | 🎯 F1 Score (±40ms): 0.1232\n",
            "📘 Epoch 260/20000000\n",
            "✅ Avg Loss: 0.426240 | 🎯 F1 Score (±40ms): 0.2230\n",
            "📘 Epoch 261/20000000\n",
            "✅ Avg Loss: 0.425572 | 🎯 F1 Score (±40ms): 0.2273\n",
            "📘 Epoch 262/20000000\n",
            "✅ Avg Loss: 0.423969 | 🎯 F1 Score (±40ms): 0.1610\n",
            "📘 Epoch 263/20000000\n",
            "✅ Avg Loss: 0.421874 | 🎯 F1 Score (±40ms): 0.1922\n",
            "📘 Epoch 264/20000000\n",
            "✅ Avg Loss: 0.423244 | 🎯 F1 Score (±40ms): 0.1390\n",
            "📘 Epoch 265/20000000\n",
            "✅ Avg Loss: 0.425447 | 🎯 F1 Score (±40ms): 0.1288\n",
            "📘 Epoch 266/20000000\n",
            "✅ Avg Loss: 0.423846 | 🎯 F1 Score (±40ms): 0.1909\n",
            "📘 Epoch 267/20000000\n",
            "✅ Avg Loss: 0.422459 | 🎯 F1 Score (±40ms): 0.2478\n",
            "📘 Epoch 268/20000000\n",
            "✅ Avg Loss: 0.422339 | 🎯 F1 Score (±40ms): 0.1786\n",
            "📘 Epoch 269/20000000\n",
            "✅ Avg Loss: 0.422740 | 🎯 F1 Score (±40ms): 0.2449\n",
            "📘 Epoch 270/20000000\n",
            "✅ Avg Loss: 0.424762 | 🎯 F1 Score (±40ms): 0.1426\n",
            "📘 Epoch 271/20000000\n",
            "✅ Avg Loss: 0.426878 | 🎯 F1 Score (±40ms): 0.2382\n",
            "📘 Epoch 272/20000000\n",
            "✅ Avg Loss: 0.426028 | 🎯 F1 Score (±40ms): 0.2338\n",
            "📘 Epoch 273/20000000\n",
            "✅ Avg Loss: 0.423551 | 🎯 F1 Score (±40ms): 0.2016\n",
            "📘 Epoch 274/20000000\n",
            "✅ Avg Loss: 0.422997 | 🎯 F1 Score (±40ms): 0.1607\n",
            "📘 Epoch 275/20000000\n",
            "✅ Avg Loss: 0.420373 | 🎯 F1 Score (±40ms): 0.1641\n",
            "📘 Epoch 276/20000000\n",
            "✅ Avg Loss: 0.423723 | 🎯 F1 Score (±40ms): 0.1640\n",
            "📘 Epoch 277/20000000\n",
            "✅ Avg Loss: 0.423080 | 🎯 F1 Score (±40ms): 0.2289\n",
            "📘 Epoch 278/20000000\n",
            "✅ Avg Loss: 0.421677 | 🎯 F1 Score (±40ms): 0.2519\n",
            "📘 Epoch 279/20000000\n",
            "✅ Avg Loss: 0.423189 | 🎯 F1 Score (±40ms): 0.1306\n",
            "📘 Epoch 280/20000000\n",
            "✅ Avg Loss: 0.423437 | 🎯 F1 Score (±40ms): 0.2503\n",
            "📘 Epoch 281/20000000\n",
            "✅ Avg Loss: 0.421883 | 🎯 F1 Score (±40ms): 0.1746\n",
            "📘 Epoch 282/20000000\n",
            "✅ Avg Loss: 0.425919 | 🎯 F1 Score (±40ms): 0.2031\n",
            "📘 Epoch 283/20000000\n",
            "✅ Avg Loss: 0.423588 | 🎯 F1 Score (±40ms): 0.1071\n",
            "📘 Epoch 284/20000000\n",
            "✅ Avg Loss: 0.422387 | 🎯 F1 Score (±40ms): 0.1851\n",
            "📘 Epoch 285/20000000\n",
            "✅ Avg Loss: 0.421755 | 🎯 F1 Score (±40ms): 0.1934\n",
            "📘 Epoch 286/20000000\n",
            "✅ Avg Loss: 0.424231 | 🎯 F1 Score (±40ms): 0.1700\n",
            "📘 Epoch 287/20000000\n",
            "✅ Avg Loss: 0.421690 | 🎯 F1 Score (±40ms): 0.1211\n",
            "📘 Epoch 288/20000000\n",
            "✅ Avg Loss: 0.424830 | 🎯 F1 Score (±40ms): 0.2331\n",
            "📘 Epoch 289/20000000\n",
            "✅ Avg Loss: 0.423533 | 🎯 F1 Score (±40ms): 0.1347\n",
            "📘 Epoch 290/20000000\n",
            "✅ Avg Loss: 0.423891 | 🎯 F1 Score (±40ms): 0.1572\n",
            "📘 Epoch 291/20000000\n",
            "✅ Avg Loss: 0.423320 | 🎯 F1 Score (±40ms): 0.2455\n",
            "📘 Epoch 292/20000000\n",
            "✅ Avg Loss: 0.421342 | 🎯 F1 Score (±40ms): 0.2228\n",
            "📘 Epoch 293/20000000\n",
            "✅ Avg Loss: 0.426042 | 🎯 F1 Score (±40ms): 0.1769\n",
            "📘 Epoch 294/20000000\n",
            "✅ Avg Loss: 0.421754 | 🎯 F1 Score (±40ms): 0.1290\n",
            "📘 Epoch 295/20000000\n",
            "✅ Avg Loss: 0.421288 | 🎯 F1 Score (±40ms): 0.1839\n",
            "📘 Epoch 296/20000000\n",
            "✅ Avg Loss: 0.421909 | 🎯 F1 Score (±40ms): 0.2074\n",
            "📘 Epoch 297/20000000\n",
            "✅ Avg Loss: 0.422838 | 🎯 F1 Score (±40ms): 0.1345\n",
            "📘 Epoch 298/20000000\n",
            "✅ Avg Loss: 0.423420 | 🎯 F1 Score (±40ms): 0.1742\n",
            "📘 Epoch 299/20000000\n",
            "✅ Avg Loss: 0.423556 | 🎯 F1 Score (±40ms): 0.1083\n",
            "📘 Epoch 300/20000000\n",
            "✅ Avg Loss: 0.421787 | 🎯 F1 Score (±40ms): 0.1239\n",
            "📘 Epoch 301/20000000\n",
            "✅ Avg Loss: 0.422257 | 🎯 F1 Score (±40ms): 0.1925\n",
            "📘 Epoch 302/20000000\n",
            "✅ Avg Loss: 0.422866 | 🎯 F1 Score (±40ms): 0.1975\n",
            "📘 Epoch 303/20000000\n",
            "✅ Avg Loss: 0.423821 | 🎯 F1 Score (±40ms): 0.0629\n",
            "📘 Epoch 304/20000000\n",
            "✅ Avg Loss: 0.426451 | 🎯 F1 Score (±40ms): 0.0919\n",
            "📘 Epoch 305/20000000\n",
            "✅ Avg Loss: 0.424908 | 🎯 F1 Score (±40ms): 0.2357\n",
            "📘 Epoch 306/20000000\n",
            "✅ Avg Loss: 0.425163 | 🎯 F1 Score (±40ms): 0.2269\n",
            "📘 Epoch 307/20000000\n",
            "✅ Avg Loss: 0.421020 | 🎯 F1 Score (±40ms): 0.2497\n",
            "📘 Epoch 308/20000000\n",
            "✅ Avg Loss: 0.425897 | 🎯 F1 Score (±40ms): 0.0899\n",
            "📘 Epoch 309/20000000\n",
            "✅ Avg Loss: 0.421273 | 🎯 F1 Score (±40ms): 0.2409\n",
            "📘 Epoch 310/20000000\n",
            "✅ Avg Loss: 0.421558 | 🎯 F1 Score (±40ms): 0.2623\n",
            "📘 Epoch 311/20000000\n",
            "✅ Avg Loss: 0.422136 | 🎯 F1 Score (±40ms): 0.2333\n",
            "📘 Epoch 312/20000000\n",
            "✅ Avg Loss: 0.424514 | 🎯 F1 Score (±40ms): 0.2241\n",
            "📘 Epoch 313/20000000\n",
            "✅ Avg Loss: 0.421506 | 🎯 F1 Score (±40ms): 0.2251\n",
            "📘 Epoch 314/20000000\n",
            "✅ Avg Loss: 0.425053 | 🎯 New Best F1: 0.2805 🏆\n",
            "💾 Model saved: onset_model_epoch314.pth\n",
            "📘 Epoch 315/20000000\n",
            "✅ Avg Loss: 0.422361 | 🎯 F1 Score (±40ms): 0.2338\n",
            "📘 Epoch 316/20000000\n",
            "✅ Avg Loss: 0.421062 | 🎯 F1 Score (±40ms): 0.2064\n",
            "📘 Epoch 317/20000000\n",
            "✅ Avg Loss: 0.422132 | 🎯 F1 Score (±40ms): 0.1925\n",
            "📘 Epoch 318/20000000\n",
            "✅ Avg Loss: 0.423632 | 🎯 F1 Score (±40ms): 0.2468\n",
            "📘 Epoch 319/20000000\n",
            "✅ Avg Loss: 0.423055 | 🎯 F1 Score (±40ms): 0.2555\n",
            "📘 Epoch 320/20000000\n",
            "✅ Avg Loss: 0.421864 | 🎯 F1 Score (±40ms): 0.1729\n",
            "📘 Epoch 321/20000000\n",
            "✅ Avg Loss: 0.421684 | 🎯 F1 Score (±40ms): 0.2336\n",
            "📘 Epoch 322/20000000\n",
            "✅ Avg Loss: 0.425085 | 🎯 F1 Score (±40ms): 0.2617\n",
            "📘 Epoch 323/20000000\n",
            "✅ Avg Loss: 0.421086 | 🎯 F1 Score (±40ms): 0.2310\n",
            "📘 Epoch 324/20000000\n",
            "✅ Avg Loss: 0.420334 | 🎯 F1 Score (±40ms): 0.2270\n",
            "📘 Epoch 325/20000000\n",
            "✅ Avg Loss: 0.422385 | 🎯 F1 Score (±40ms): 0.1719\n",
            "📘 Epoch 326/20000000\n",
            "✅ Avg Loss: 0.426377 | 🎯 F1 Score (±40ms): 0.2521\n",
            "📘 Epoch 327/20000000\n",
            "✅ Avg Loss: 0.424028 | 🎯 F1 Score (±40ms): 0.2782\n",
            "📘 Epoch 328/20000000\n",
            "✅ Avg Loss: 0.423444 | 🎯 F1 Score (±40ms): 0.1961\n",
            "📘 Epoch 329/20000000\n",
            "✅ Avg Loss: 0.422178 | 🎯 F1 Score (±40ms): 0.1783\n",
            "📘 Epoch 330/20000000\n",
            "✅ Avg Loss: 0.421546 | 🎯 F1 Score (±40ms): 0.2119\n",
            "📘 Epoch 331/20000000\n",
            "✅ Avg Loss: 0.422840 | 🎯 F1 Score (±40ms): 0.2356\n",
            "📘 Epoch 332/20000000\n",
            "✅ Avg Loss: 0.423422 | 🎯 F1 Score (±40ms): 0.1345\n",
            "📘 Epoch 333/20000000\n",
            "✅ Avg Loss: 0.422406 | 🎯 F1 Score (±40ms): 0.2172\n",
            "📘 Epoch 334/20000000\n",
            "✅ Avg Loss: 0.424087 | 🎯 F1 Score (±40ms): 0.2331\n",
            "📘 Epoch 335/20000000\n",
            "✅ Avg Loss: 0.423915 | 🎯 F1 Score (±40ms): 0.0850\n",
            "📘 Epoch 336/20000000\n",
            "✅ Avg Loss: 0.425137 | 🎯 F1 Score (±40ms): 0.1179\n",
            "📘 Epoch 337/20000000\n",
            "✅ Avg Loss: 0.422089 | 🎯 F1 Score (±40ms): 0.2772\n",
            "📘 Epoch 338/20000000\n",
            "✅ Avg Loss: 0.424341 | 🎯 F1 Score (±40ms): 0.1680\n",
            "📘 Epoch 339/20000000\n",
            "✅ Avg Loss: 0.422785 | 🎯 F1 Score (±40ms): 0.2245\n",
            "📘 Epoch 340/20000000\n",
            "✅ Avg Loss: 0.421488 | 🎯 F1 Score (±40ms): 0.1992\n",
            "📘 Epoch 341/20000000\n",
            "✅ Avg Loss: 0.422837 | 🎯 F1 Score (±40ms): 0.1869\n",
            "📘 Epoch 342/20000000\n",
            "✅ Avg Loss: 0.421569 | 🎯 F1 Score (±40ms): 0.1129\n",
            "📘 Epoch 343/20000000\n",
            "✅ Avg Loss: 0.420572 | 🎯 F1 Score (±40ms): 0.2503\n",
            "📘 Epoch 344/20000000\n",
            "✅ Avg Loss: 0.426780 | 🎯 F1 Score (±40ms): 0.2119\n",
            "📘 Epoch 345/20000000\n",
            "✅ Avg Loss: 0.422649 | 🎯 F1 Score (±40ms): 0.2678\n",
            "📘 Epoch 346/20000000\n",
            "✅ Avg Loss: 0.424383 | 🎯 F1 Score (±40ms): 0.2367\n",
            "📘 Epoch 347/20000000\n",
            "✅ Avg Loss: 0.421496 | 🎯 F1 Score (±40ms): 0.1797\n",
            "📘 Epoch 348/20000000\n",
            "✅ Avg Loss: 0.420649 | 🎯 F1 Score (±40ms): 0.1562\n",
            "📘 Epoch 349/20000000\n",
            "✅ Avg Loss: 0.421225 | 🎯 F1 Score (±40ms): 0.2143\n",
            "📘 Epoch 350/20000000\n",
            "✅ Avg Loss: 0.420449 | 🎯 F1 Score (±40ms): 0.1336\n",
            "📘 Epoch 351/20000000\n",
            "✅ Avg Loss: 0.422715 | 🎯 F1 Score (±40ms): 0.2261\n",
            "📘 Epoch 352/20000000\n",
            "✅ Avg Loss: 0.424303 | 🎯 F1 Score (±40ms): 0.1785\n",
            "📘 Epoch 353/20000000\n",
            "✅ Avg Loss: 0.423140 | 🎯 F1 Score (±40ms): 0.1455\n",
            "📘 Epoch 354/20000000\n",
            "✅ Avg Loss: 0.422235 | 🎯 F1 Score (±40ms): 0.2321\n",
            "📘 Epoch 355/20000000\n",
            "✅ Avg Loss: 0.423591 | 🎯 F1 Score (±40ms): 0.1519\n",
            "📘 Epoch 356/20000000\n",
            "✅ Avg Loss: 0.422910 | 🎯 F1 Score (±40ms): 0.1405\n",
            "📘 Epoch 357/20000000\n",
            "✅ Avg Loss: 0.423033 | 🎯 F1 Score (±40ms): 0.2699\n",
            "📘 Epoch 358/20000000\n",
            "✅ Avg Loss: 0.419324 | 🎯 F1 Score (±40ms): 0.2249\n",
            "📘 Epoch 359/20000000\n",
            "✅ Avg Loss: 0.421979 | 🎯 F1 Score (±40ms): 0.2738\n",
            "📘 Epoch 360/20000000\n",
            "✅ Avg Loss: 0.421911 | 🎯 F1 Score (±40ms): 0.2437\n",
            "📘 Epoch 361/20000000\n",
            "✅ Avg Loss: 0.419828 | 🎯 F1 Score (±40ms): 0.1992\n",
            "📘 Epoch 362/20000000\n",
            "✅ Avg Loss: 0.424101 | 🎯 F1 Score (±40ms): 0.0912\n",
            "📘 Epoch 363/20000000\n",
            "✅ Avg Loss: 0.421469 | 🎯 F1 Score (±40ms): 0.1259\n",
            "📘 Epoch 364/20000000\n",
            "✅ Avg Loss: 0.422770 | 🎯 F1 Score (±40ms): 0.1119\n",
            "📘 Epoch 365/20000000\n",
            "✅ Avg Loss: 0.422074 | 🎯 F1 Score (±40ms): 0.2140\n",
            "📘 Epoch 366/20000000\n",
            "✅ Avg Loss: 0.422224 | 🎯 F1 Score (±40ms): 0.2179\n",
            "📘 Epoch 367/20000000\n",
            "✅ Avg Loss: 0.420639 | 🎯 F1 Score (±40ms): 0.2790\n",
            "📘 Epoch 368/20000000\n",
            "✅ Avg Loss: 0.423429 | 🎯 F1 Score (±40ms): 0.2366\n",
            "📘 Epoch 369/20000000\n",
            "✅ Avg Loss: 0.425337 | 🎯 F1 Score (±40ms): 0.1568\n",
            "📘 Epoch 370/20000000\n",
            "✅ Avg Loss: 0.423201 | 🎯 F1 Score (±40ms): 0.1909\n",
            "📘 Epoch 371/20000000\n",
            "✅ Avg Loss: 0.425942 | 🎯 F1 Score (±40ms): 0.2621\n",
            "📘 Epoch 372/20000000\n",
            "✅ Avg Loss: 0.421493 | 🎯 F1 Score (±40ms): 0.2003\n",
            "📘 Epoch 373/20000000\n",
            "✅ Avg Loss: 0.421039 | 🎯 F1 Score (±40ms): 0.2354\n",
            "📘 Epoch 374/20000000\n",
            "✅ Avg Loss: 0.420946 | 🎯 F1 Score (±40ms): 0.2390\n",
            "📘 Epoch 375/20000000\n",
            "✅ Avg Loss: 0.420827 | 🎯 F1 Score (±40ms): 0.0768\n",
            "📘 Epoch 376/20000000\n",
            "✅ Avg Loss: 0.423793 | 🎯 F1 Score (±40ms): 0.1457\n",
            "📘 Epoch 377/20000000\n",
            "✅ Avg Loss: 0.423478 | 🎯 F1 Score (±40ms): 0.2045\n",
            "📘 Epoch 378/20000000\n",
            "✅ Avg Loss: 0.420928 | 🎯 F1 Score (±40ms): 0.1653\n",
            "📘 Epoch 379/20000000\n",
            "✅ Avg Loss: 0.423261 | 🎯 F1 Score (±40ms): 0.1125\n",
            "📘 Epoch 380/20000000\n",
            "✅ Avg Loss: 0.422512 | 🎯 F1 Score (±40ms): 0.1832\n",
            "📘 Epoch 381/20000000\n",
            "✅ Avg Loss: 0.422757 | 🎯 F1 Score (±40ms): 0.2337\n",
            "📘 Epoch 382/20000000\n",
            "✅ Avg Loss: 0.424626 | 🎯 F1 Score (±40ms): 0.2062\n",
            "📘 Epoch 383/20000000\n",
            "✅ Avg Loss: 0.423672 | 🎯 F1 Score (±40ms): 0.0663\n",
            "📘 Epoch 384/20000000\n",
            "✅ Avg Loss: 0.422737 | 🎯 F1 Score (±40ms): 0.2336\n",
            "📘 Epoch 385/20000000\n",
            "✅ Avg Loss: 0.420997 | 🎯 F1 Score (±40ms): 0.0959\n",
            "📘 Epoch 386/20000000\n",
            "✅ Avg Loss: 0.425620 | 🎯 F1 Score (±40ms): 0.1772\n",
            "📘 Epoch 387/20000000\n",
            "✅ Avg Loss: 0.420746 | 🎯 F1 Score (±40ms): 0.2706\n",
            "📘 Epoch 388/20000000\n",
            "✅ Avg Loss: 0.421850 | 🎯 F1 Score (±40ms): 0.2639\n",
            "📘 Epoch 389/20000000\n",
            "✅ Avg Loss: 0.420841 | 🎯 F1 Score (±40ms): 0.2474\n",
            "📘 Epoch 390/20000000\n",
            "✅ Avg Loss: 0.423237 | 🎯 F1 Score (±40ms): 0.1682\n",
            "📘 Epoch 391/20000000\n",
            "✅ Avg Loss: 0.419544 | 🎯 F1 Score (±40ms): 0.2092\n",
            "📘 Epoch 392/20000000\n",
            "✅ Avg Loss: 0.422217 | 🎯 F1 Score (±40ms): 0.2487\n",
            "📘 Epoch 393/20000000\n",
            "✅ Avg Loss: 0.425038 | 🎯 F1 Score (±40ms): 0.1971\n",
            "📘 Epoch 394/20000000\n",
            "✅ Avg Loss: 0.422723 | 🎯 F1 Score (±40ms): 0.0835\n",
            "📘 Epoch 395/20000000\n",
            "✅ Avg Loss: 0.422377 | 🎯 F1 Score (±40ms): 0.2118\n",
            "📘 Epoch 396/20000000\n",
            "✅ Avg Loss: 0.419630 | 🎯 F1 Score (±40ms): 0.1265\n",
            "📘 Epoch 397/20000000\n",
            "✅ Avg Loss: 0.426893 | 🎯 F1 Score (±40ms): 0.1093\n",
            "📘 Epoch 398/20000000\n",
            "✅ Avg Loss: 0.423979 | 🎯 F1 Score (±40ms): 0.2241\n",
            "📘 Epoch 399/20000000\n",
            "✅ Avg Loss: 0.421374 | 🎯 F1 Score (±40ms): 0.1766\n",
            "📘 Epoch 400/20000000\n",
            "✅ Avg Loss: 0.422221 | 🎯 F1 Score (±40ms): 0.1786\n",
            "📘 Epoch 401/20000000\n",
            "✅ Avg Loss: 0.421560 | 🎯 F1 Score (±40ms): 0.1331\n",
            "📘 Epoch 402/20000000\n",
            "✅ Avg Loss: 0.421051 | 🎯 F1 Score (±40ms): 0.2490\n",
            "📘 Epoch 403/20000000\n",
            "✅ Avg Loss: 0.419663 | 🎯 F1 Score (±40ms): 0.1347\n",
            "📘 Epoch 404/20000000\n",
            "✅ Avg Loss: 0.421262 | 🎯 F1 Score (±40ms): 0.2372\n",
            "📘 Epoch 405/20000000\n",
            "✅ Avg Loss: 0.422258 | 🎯 F1 Score (±40ms): 0.2651\n",
            "📘 Epoch 406/20000000\n",
            "✅ Avg Loss: 0.423117 | 🎯 F1 Score (±40ms): 0.2064\n",
            "📘 Epoch 407/20000000\n",
            "✅ Avg Loss: 0.422390 | 🎯 F1 Score (±40ms): 0.1336\n",
            "📘 Epoch 408/20000000\n",
            "✅ Avg Loss: 0.421935 | 🎯 F1 Score (±40ms): 0.1848\n",
            "📘 Epoch 409/20000000\n",
            "✅ Avg Loss: 0.421622 | 🎯 F1 Score (±40ms): 0.2610\n",
            "📘 Epoch 410/20000000\n",
            "✅ Avg Loss: 0.419796 | 🎯 F1 Score (±40ms): 0.1254\n",
            "📘 Epoch 411/20000000\n",
            "✅ Avg Loss: 0.420424 | 🎯 F1 Score (±40ms): 0.1964\n",
            "📘 Epoch 412/20000000\n",
            "✅ Avg Loss: 0.425746 | 🎯 F1 Score (±40ms): 0.1627\n",
            "📘 Epoch 413/20000000\n",
            "✅ Avg Loss: 0.423009 | 🎯 F1 Score (±40ms): 0.2309\n",
            "📘 Epoch 414/20000000\n",
            "✅ Avg Loss: 0.422328 | 🎯 F1 Score (±40ms): 0.2377\n",
            "📘 Epoch 415/20000000\n",
            "✅ Avg Loss: 0.422722 | 🎯 F1 Score (±40ms): 0.2432\n",
            "📘 Epoch 416/20000000\n",
            "✅ Avg Loss: 0.422028 | 🎯 F1 Score (±40ms): 0.2680\n",
            "📘 Epoch 417/20000000\n",
            "✅ Avg Loss: 0.420630 | 🎯 F1 Score (±40ms): 0.2398\n",
            "📘 Epoch 418/20000000\n",
            "✅ Avg Loss: 0.420072 | 🎯 F1 Score (±40ms): 0.2357\n",
            "📘 Epoch 419/20000000\n",
            "✅ Avg Loss: 0.422683 | 🎯 F1 Score (±40ms): 0.1282\n",
            "📘 Epoch 420/20000000\n",
            "✅ Avg Loss: 0.420648 | 🎯 F1 Score (±40ms): 0.1960\n",
            "📘 Epoch 421/20000000\n",
            "✅ Avg Loss: 0.421163 | 🎯 F1 Score (±40ms): 0.1960\n",
            "📘 Epoch 422/20000000\n",
            "✅ Avg Loss: 0.419893 | 🎯 F1 Score (±40ms): 0.1960\n",
            "📘 Epoch 423/20000000\n",
            "✅ Avg Loss: 0.422030 | 🎯 F1 Score (±40ms): 0.1060\n",
            "📘 Epoch 424/20000000\n",
            "✅ Avg Loss: 0.420386 | 🎯 F1 Score (±40ms): 0.2658\n",
            "📘 Epoch 425/20000000\n",
            "✅ Avg Loss: 0.420822 | 🎯 F1 Score (±40ms): 0.1876\n",
            "📘 Epoch 426/20000000\n",
            "✅ Avg Loss: 0.423930 | 🎯 F1 Score (±40ms): 0.2268\n",
            "📘 Epoch 427/20000000\n",
            "✅ Avg Loss: 0.420570 | 🎯 F1 Score (±40ms): 0.2682\n",
            "📘 Epoch 428/20000000\n",
            "✅ Avg Loss: 0.419799 | 🎯 F1 Score (±40ms): 0.2406\n",
            "📘 Epoch 429/20000000\n",
            "✅ Avg Loss: 0.420077 | 🎯 F1 Score (±40ms): 0.2468\n",
            "📘 Epoch 430/20000000\n",
            "✅ Avg Loss: 0.421754 | 🎯 F1 Score (±40ms): 0.2770\n",
            "📘 Epoch 431/20000000\n",
            "✅ Avg Loss: 0.421711 | 🎯 F1 Score (±40ms): 0.1790\n",
            "📘 Epoch 432/20000000\n",
            "✅ Avg Loss: 0.421127 | 🎯 F1 Score (±40ms): 0.2592\n",
            "📘 Epoch 433/20000000\n",
            "✅ Avg Loss: 0.421439 | 🎯 F1 Score (±40ms): 0.2462\n",
            "📘 Epoch 434/20000000\n",
            "✅ Avg Loss: 0.423220 | 🎯 F1 Score (±40ms): 0.1635\n",
            "📘 Epoch 435/20000000\n",
            "✅ Avg Loss: 0.421097 | 🎯 F1 Score (±40ms): 0.2113\n",
            "📘 Epoch 436/20000000\n",
            "✅ Avg Loss: 0.420457 | 🎯 F1 Score (±40ms): 0.2170\n",
            "📘 Epoch 437/20000000\n",
            "✅ Avg Loss: 0.423302 | 🎯 F1 Score (±40ms): 0.2613\n",
            "📘 Epoch 438/20000000\n",
            "✅ Avg Loss: 0.422198 | 🎯 F1 Score (±40ms): 0.2563\n",
            "📘 Epoch 439/20000000\n",
            "✅ Avg Loss: 0.425896 | 🎯 F1 Score (±40ms): 0.1630\n",
            "📘 Epoch 440/20000000\n",
            "✅ Avg Loss: 0.421650 | 🎯 F1 Score (±40ms): 0.2361\n",
            "📘 Epoch 441/20000000\n",
            "✅ Avg Loss: 0.421523 | 🎯 F1 Score (±40ms): 0.1785\n",
            "📘 Epoch 442/20000000\n",
            "✅ Avg Loss: 0.419952 | 🎯 F1 Score (±40ms): 0.1899\n",
            "📘 Epoch 443/20000000\n",
            "✅ Avg Loss: 0.422626 | 🎯 F1 Score (±40ms): 0.2437\n",
            "📘 Epoch 444/20000000\n",
            "✅ Avg Loss: 0.420184 | 🎯 F1 Score (±40ms): 0.2715\n",
            "📘 Epoch 445/20000000\n",
            "✅ Avg Loss: 0.423010 | 🎯 F1 Score (±40ms): 0.2244\n",
            "📘 Epoch 446/20000000\n",
            "✅ Avg Loss: 0.419934 | 🎯 F1 Score (±40ms): 0.2802\n",
            "📘 Epoch 447/20000000\n",
            "✅ Avg Loss: 0.422311 | 🎯 F1 Score (±40ms): 0.2549\n",
            "📘 Epoch 448/20000000\n",
            "✅ Avg Loss: 0.422047 | 🎯 F1 Score (±40ms): 0.1975\n",
            "📘 Epoch 449/20000000\n",
            "✅ Avg Loss: 0.421017 | 🎯 F1 Score (±40ms): 0.2161\n",
            "📘 Epoch 450/20000000\n",
            "✅ Avg Loss: 0.420502 | 🎯 F1 Score (±40ms): 0.2674\n",
            "📘 Epoch 451/20000000\n",
            "✅ Avg Loss: 0.421217 | 🎯 F1 Score (±40ms): 0.2376\n",
            "📘 Epoch 452/20000000\n",
            "✅ Avg Loss: 0.421262 | 🎯 F1 Score (±40ms): 0.2581\n",
            "📘 Epoch 453/20000000\n",
            "✅ Avg Loss: 0.419500 | 🎯 F1 Score (±40ms): 0.2494\n",
            "📘 Epoch 454/20000000\n",
            "✅ Avg Loss: 0.419678 | 🎯 F1 Score (±40ms): 0.2022\n",
            "📘 Epoch 455/20000000\n",
            "✅ Avg Loss: 0.422421 | 🎯 F1 Score (±40ms): 0.1414\n",
            "📘 Epoch 456/20000000\n",
            "✅ Avg Loss: 0.423413 | 🎯 F1 Score (±40ms): 0.1322\n",
            "📘 Epoch 457/20000000\n",
            "✅ Avg Loss: 0.421571 | 🎯 F1 Score (±40ms): 0.2513\n",
            "📘 Epoch 458/20000000\n",
            "✅ Avg Loss: 0.419330 | 🎯 New Best F1: 0.2888 🏆\n",
            "💾 Model saved: onset_model_epoch458.pth\n",
            "📘 Epoch 459/20000000\n",
            "✅ Avg Loss: 0.421992 | 🎯 F1 Score (±40ms): 0.2694\n",
            "📘 Epoch 460/20000000\n",
            "✅ Avg Loss: 0.421856 | 🎯 F1 Score (±40ms): 0.2089\n",
            "📘 Epoch 461/20000000\n",
            "✅ Avg Loss: 0.418969 | 🎯 F1 Score (±40ms): 0.1119\n",
            "📘 Epoch 462/20000000\n",
            "✅ Avg Loss: 0.421064 | 🎯 F1 Score (±40ms): 0.2437\n",
            "📘 Epoch 463/20000000\n",
            "✅ Avg Loss: 0.419757 | 🎯 F1 Score (±40ms): 0.2498\n",
            "📘 Epoch 464/20000000\n",
            "✅ Avg Loss: 0.420826 | 🎯 F1 Score (±40ms): 0.2683\n",
            "📘 Epoch 465/20000000\n",
            "✅ Avg Loss: 0.422773 | 🎯 F1 Score (±40ms): 0.2296\n",
            "📘 Epoch 466/20000000\n",
            "✅ Avg Loss: 0.419751 | 🎯 F1 Score (±40ms): 0.2693\n",
            "📘 Epoch 467/20000000\n",
            "✅ Avg Loss: 0.421978 | 🎯 F1 Score (±40ms): 0.2196\n",
            "📘 Epoch 468/20000000\n",
            "✅ Avg Loss: 0.420388 | 🎯 F1 Score (±40ms): 0.1750\n",
            "📘 Epoch 469/20000000\n",
            "✅ Avg Loss: 0.421022 | 🎯 F1 Score (±40ms): 0.2000\n",
            "📘 Epoch 470/20000000\n",
            "✅ Avg Loss: 0.420431 | 🎯 F1 Score (±40ms): 0.2272\n",
            "📘 Epoch 471/20000000\n",
            "✅ Avg Loss: 0.419578 | 🎯 F1 Score (±40ms): 0.2709\n",
            "📘 Epoch 472/20000000\n",
            "✅ Avg Loss: 0.422719 | 🎯 F1 Score (±40ms): 0.2431\n",
            "📘 Epoch 473/20000000\n",
            "✅ Avg Loss: 0.418620 | 🎯 F1 Score (±40ms): 0.2272\n",
            "📘 Epoch 474/20000000\n",
            "✅ Avg Loss: 0.420868 | 🎯 F1 Score (±40ms): 0.1862\n",
            "📘 Epoch 475/20000000\n",
            "✅ Avg Loss: 0.421929 | 🎯 F1 Score (±40ms): 0.2310\n",
            "📘 Epoch 476/20000000\n",
            "✅ Avg Loss: 0.423056 | 🎯 F1 Score (±40ms): 0.2492\n",
            "📘 Epoch 477/20000000\n",
            "✅ Avg Loss: 0.422323 | 🎯 F1 Score (±40ms): 0.1874\n",
            "📘 Epoch 478/20000000\n",
            "✅ Avg Loss: 0.419847 | 🎯 F1 Score (±40ms): 0.2095\n",
            "📘 Epoch 479/20000000\n",
            "✅ Avg Loss: 0.421448 | 🎯 F1 Score (±40ms): 0.2552\n",
            "📘 Epoch 480/20000000\n",
            "✅ Avg Loss: 0.421465 | 🎯 F1 Score (±40ms): 0.2555\n",
            "📘 Epoch 481/20000000\n",
            "✅ Avg Loss: 0.422702 | 🎯 F1 Score (±40ms): 0.2690\n",
            "📘 Epoch 482/20000000\n",
            "✅ Avg Loss: 0.424180 | 🎯 F1 Score (±40ms): 0.2615\n",
            "📘 Epoch 483/20000000\n",
            "✅ Avg Loss: 0.421096 | 🎯 F1 Score (±40ms): 0.2743\n",
            "📘 Epoch 484/20000000\n",
            "✅ Avg Loss: 0.420537 | 🎯 F1 Score (±40ms): 0.2625\n",
            "📘 Epoch 485/20000000\n",
            "✅ Avg Loss: 0.419781 | 🎯 F1 Score (±40ms): 0.2055\n",
            "📘 Epoch 486/20000000\n",
            "✅ Avg Loss: 0.419630 | 🎯 F1 Score (±40ms): 0.2247\n",
            "📘 Epoch 487/20000000\n",
            "✅ Avg Loss: 0.424608 | 🎯 F1 Score (±40ms): 0.2637\n",
            "📘 Epoch 488/20000000\n",
            "✅ Avg Loss: 0.423535 | 🎯 F1 Score (±40ms): 0.2039\n",
            "📘 Epoch 489/20000000\n",
            "✅ Avg Loss: 0.422928 | 🎯 F1 Score (±40ms): 0.2636\n",
            "📘 Epoch 490/20000000\n",
            "✅ Avg Loss: 0.421992 | 🎯 F1 Score (±40ms): 0.2721\n",
            "📘 Epoch 491/20000000\n",
            "✅ Avg Loss: 0.419584 | 🎯 F1 Score (±40ms): 0.2606\n",
            "📘 Epoch 492/20000000\n",
            "✅ Avg Loss: 0.423591 | 🎯 F1 Score (±40ms): 0.2251\n",
            "📘 Epoch 493/20000000\n",
            "✅ Avg Loss: 0.422933 | 🎯 F1 Score (±40ms): 0.2694\n",
            "📘 Epoch 494/20000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "# === CONFIG ===\n",
        "AUDIO_PATH = \"trainingaudio.mp3\"\n",
        "LABEL_PATH = \"trainingonsets.txt\"\n",
        "SR = 22050\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 32\n",
        "CONTEXT = 7\n",
        "EPOCHS = 20000000\n",
        "BATCH_SIZE = 64\n",
        "LR = 1e-2\n",
        "TOLERANCE = 0.04  # seconds\n",
        "\n",
        "# === Load audio and extract features ===\n",
        "y, sr = librosa.load(AUDIO_PATH, sr=SR)\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
        "log_S = librosa.power_to_db(S, ref=np.max).T\n",
        "\n",
        "# === Load onset labels ===\n",
        "onsets = np.array([float(l.strip()) for l in open(LABEL_PATH) if l.strip()])\n",
        "onset_frames = librosa.time_to_frames(onsets, sr=sr, hop_length=HOP_LENGTH)\n",
        "labels = np.zeros(log_S.shape[0])\n",
        "for f in onset_frames:\n",
        "    start = max(0, f - int(TOLERANCE * sr / HOP_LENGTH))\n",
        "    end = min(len(labels), f + int(TOLERANCE * sr / HOP_LENGTH) + 1)\n",
        "    labels[start:end] = 1\n",
        "\n",
        "# === Dataset ===\n",
        "class OnsetDataset(Dataset):\n",
        "    def __init__(self, X, y, context):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.context = context\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X) - 2 * self.context\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        i = idx + self.context\n",
        "        x = self.X[i - self.context:i + self.context + 1].T\n",
        "        x = np.expand_dims(x, 0)  # [1, mel, time]\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(self.y[i], dtype=torch.float32)\n",
        "\n",
        "# === Smaller CNN ===\n",
        "class SmallOnsetCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Train ===\n",
        "dataset = OnsetDataset(log_S, labels, CONTEXT)\n",
        "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SmallOnsetCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# === Load correct onset times once (as floats in seconds) ===\n",
        "with open(\"evaluateonsets.txt\") as f:\n",
        "    correct_onsets = np.array([\n",
        "        float(line.strip())\n",
        "        for line in f if line.strip() != ''\n",
        "    ])\n",
        "\n",
        "# === Load evaluation audio features from vocal.mp3 ===\n",
        "EVAL_AUDIO_PATH = \"vocal.mp3\"\n",
        "y_eval, _ = librosa.load(EVAL_AUDIO_PATH, sr=SR)\n",
        "S_eval = librosa.feature.melspectrogram(y=y_eval, sr=SR, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
        "log_S_eval = librosa.power_to_db(S_eval, ref=np.max).T\n",
        "\n",
        "# === F1 Scoring Function with Time Tolerance ===\n",
        "def tolerant_f1(model, log_S, correct_onsets, context, device, sr, hop_length, threshold=0.5, tolerance=0.05):\n",
        "    model.eval()\n",
        "    X = []\n",
        "    frame_times = []\n",
        "\n",
        "    for i in range(context, len(log_S) - context):\n",
        "        segment = log_S[i - context:i + context + 1].T\n",
        "        X.append(np.expand_dims(segment, 0))\n",
        "        frame_time = librosa.frames_to_time(i, sr=sr, hop_length=hop_length)\n",
        "        frame_times.append(frame_time)\n",
        "\n",
        "    X = torch.tensor(np.array(X), dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        preds = model(X).squeeze().cpu().numpy()\n",
        "\n",
        "    pred_times = [t for t, p in zip(frame_times, preds) if p >= threshold]\n",
        "\n",
        "    # === Match predictions to ground-truth with tolerance ===\n",
        "    matched_pred = set()\n",
        "    matched_true = set()\n",
        "\n",
        "    for i, true_onset in enumerate(correct_onsets):\n",
        "        for j, pred_onset in enumerate(pred_times):\n",
        "            if j in matched_pred:\n",
        "                continue\n",
        "            if abs(pred_onset - true_onset) <= tolerance:\n",
        "                matched_true.add(i)\n",
        "                matched_pred.add(j)\n",
        "                break\n",
        "\n",
        "    tp = len(matched_true)\n",
        "    fp = len(pred_times) - tp\n",
        "    fn = len(correct_onsets) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    return f1\n",
        "\n",
        "# === Training Loop ===\n",
        "print(\"🚀 Starting training...\")\n",
        "best_f1 = 0.0\n",
        "loss_per_epoch = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    print(f\"📘 Epoch {epoch+1}/{EPOCHS}\")\n",
        "    for batch_idx, (x, y) in enumerate(loader):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        pred = model(x).squeeze()\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(dataset)\n",
        "    loss_per_epoch.append(avg_loss)\n",
        "\n",
        "    # === Compute F1 Score using vocal.mp3 ===\n",
        "    f1 = tolerant_f1(model, log_S_eval, correct_onsets, CONTEXT, device, SR, HOP_LENGTH)\n",
        "\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        print(f\"✅ Avg Loss: {avg_loss:.6f} | 🎯 New Best F1: {f1:.4f} 🏆\")\n",
        "        # Save checkpoint\n",
        "        model_path = f\"onset_model_epoch{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"💾 Model saved: {model_path}\")\n",
        "    else:\n",
        "        print(f\"✅ Avg Loss: {avg_loss:.6f} | 🎯 F1 Score (±40ms): {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "e-YbNe4sjU3h",
        "outputId": "cd9a5357-7ec9-4356-8749-834cc6f42301"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4-3232997049.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfull_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_mp3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_audio_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpreview_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_audio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreview_duration_ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpreview_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreview_output_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mp3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_mp3\u001b[0;34m(cls, file, parameters)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_mp3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mp3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                     p.returncode, p_err.decode(errors='ignore') ))\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0mfix_wav_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "# === PREVIEW CONFIG ===\n",
        "input_audio_path = \"melody.mp3\"          # 👈 Your full audio file\n",
        "preview_output_path = \"preview.mp3\"\n",
        "preview_duration_ms = 10 * 60 * 1000     # 10 minutes\n",
        "\n",
        "# === CREATE PREVIEW ===\n",
        "from pydub import AudioSegment\n",
        "\n",
        "full_audio = AudioSegment.from_mp3(input_audio_path)\n",
        "preview_audio = full_audio[:preview_duration_ms]\n",
        "preview_audio.export(preview_output_path, format=\"mp3\")\n",
        "\n",
        "print(f\"🎧 Preview saved to: {preview_output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vUpZ2VyuSgG",
        "outputId": "e5e2e727-08a1-4d21-d5ac-041057273031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting midiutil\n",
            "  Downloading MIDIUtil-1.2.1.tar.gz (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (2.0.2)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mido>=1.1.16->pretty_midi) (25.0)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: midiutil, pretty_midi\n",
            "  Building wheel for midiutil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for midiutil: filename=MIDIUtil-1.2.1-py3-none-any.whl size=54569 sha256=bcf05634758a62360fdcdc6c28a4765872cf33ecf5b4f195dc270273e1840518\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/42/75/fce10c67f06fe627fad8acd1fd3a004a24e07b0f077761fbbd\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=355347e0ef7d6b2c9c2e29981bdbbe0e7aef2d311041b8d020b6aed88a95f1b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built midiutil pretty_midi\n",
            "Installing collected packages: midiutil, mido, pretty_midi\n",
            "Successfully installed midiutil-1.2.1 mido-1.3.3 pretty_midi-0.2.10\n"
          ]
        }
      ],
      "source": [
        "!pip install midiutil pretty_midi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F75oplE-uOrA",
        "outputId": "7d8f70ba-3dba-4785-f6ec-03dfded24e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎯 Evaluation F1 Score (±50ms): 0.1218\n",
            "   Precision: 0.0794 | Recall: 0.2612 | TP: 117, FP: 1356, FN: 331\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.12181155290324952"
            ]
          },
          "execution_count": 169,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# === CONFIG ===\n",
        "AUDIO_PATH = \"vocal.mp3\"\n",
        "MODEL_PATH = \"Onset.pth\"\n",
        "LABEL_PATH = \"evaluateonsets.txt\"\n",
        "SR = 22050\n",
        "HOP_LENGTH = 512\n",
        "N_MELS = 32\n",
        "CONTEXT = 7\n",
        "THRESHOLD = 0.5\n",
        "TOLERANCE = 0.05  # seconds\n",
        "\n",
        "# === Model Definition ===\n",
        "class SmallOnsetCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# === Load model ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SmallOnsetCNN().to(device)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# === Load audio and features ===\n",
        "y, sr = librosa.load(AUDIO_PATH, sr=SR)\n",
        "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
        "log_S = librosa.power_to_db(S, ref=np.max).T\n",
        "\n",
        "# === Load ground-truth onsets (in seconds) ===\n",
        "with open(LABEL_PATH) as f:\n",
        "    correct_onsets = np.array([float(line.strip()) for line in f if line.strip()])\n",
        "\n",
        "# === Evaluation Function ===\n",
        "def tolerant_f1(model, log_S, correct_onsets, context, device, sr, hop_length, threshold=0.5, tolerance=0.05):\n",
        "    model.eval()\n",
        "    X = []\n",
        "    frame_times = []\n",
        "\n",
        "    for i in range(context, len(log_S) - context):\n",
        "        segment = log_S[i - context:i + context + 1].T\n",
        "        X.append(np.expand_dims(segment, 0))\n",
        "        frame_time = librosa.frames_to_time(i, sr=sr, hop_length=hop_length)\n",
        "        frame_times.append(frame_time)\n",
        "\n",
        "    X = torch.tensor(np.array(X), dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        preds = model(X).squeeze().cpu().numpy()\n",
        "\n",
        "    pred_times = [t for t, p in zip(frame_times, preds) if p >= threshold]\n",
        "\n",
        "    matched_pred = set()\n",
        "    matched_true = set()\n",
        "\n",
        "    for i, true_onset in enumerate(correct_onsets):\n",
        "        for j, pred_onset in enumerate(pred_times):\n",
        "            if j in matched_pred:\n",
        "                continue\n",
        "            if abs(pred_onset - true_onset) <= tolerance:\n",
        "                matched_true.add(i)\n",
        "                matched_pred.add(j)\n",
        "                break\n",
        "\n",
        "    tp = len(matched_true)\n",
        "    fp = len(pred_times) - tp\n",
        "    fn = len(correct_onsets) - tp\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    print(f\"🎯 Evaluation F1 Score (±{tolerance*1000:.0f}ms): {f1:.4f}\")\n",
        "    print(f\"   Precision: {precision:.4f} | Recall: {recall:.4f} | TP: {tp}, FP: {fp}, FN: {fn}\")\n",
        "    return f1\n",
        "\n",
        "# === Run Evaluation ===\n",
        "tolerant_f1(model, log_S, correct_onsets, CONTEXT, device, SR, HOP_LENGTH, THRESHOLD, TOLERANCE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}